\chapter[Ideology]{Ideology\footnotemark}
\footnotetext{Or, at least, my understanding of a software development ideology
    I believe to be relevant to this work and Drasil.}
\label{chap:ideology}

``Generation'' is at the heart of this work. However, unlike GitHub and OpenAI's
Copilot \cite{Copilot}, we don't delve into artificial intelligence. Copilot
uses AI to autocomplete code using smaller snippets and comments, while we focus
on capturing the meaning of specific subsets of language to generate software
artifacts through description of their requirements. In other words, we focus on
encoding the important bits of information that we use to discuss requirements
of software and how they relate to the software artifacts we would normally
manually write. We focus on capturing knowledge through codifying
well-understood fragments with \acsp{dsl} to capture the \textit{harmonious
relationship} between \textit{knowledge} and \textit{software artifacts} to
improve software \textit{maintainability} and knowledge \textit{communication}
and \textit{reusability}.

\section{On Developing Software}
\label{chap:ideology:sec:on_developing_software}

As software developers, we encode ``stories'' through software. As an example,
programs made to process \acs{csv} files tell a story of how data can be
entered, adjusted, and output. Compilers tell a story of how human-readable
programs can translate to various assembly languages. In these stories, we
commonly use similar terminology and ideas. Thankfully, we avoid writing ``a lot
of the same code'' by abstracting over variables and sharing reusable code
through libraries. With libraries, we're able to share our knowledge with our
future selves and others alike. Once we've reasonably stabilized our libraries,
it becomes a large gain in the reusability of our efforts \textemdash we don't
need to worry about making the same bugs twice! However, a few issues arise: the
code might become out-of-date (or out-of-sync), others might not understand what
we wrote (and thus not trust/use it), or we might need to use the same
conceptual ideas but in a different programming language. If our understanding
of key ideas changes, we might need to perform large, manual refactoring of our
code, and also update our documentation too. While we might write ``idiomatic''
code, reverse engineering code is tedious, and even then, we can only analyze
the code we \textit{see}, and not what knowledge it took to write that code.
Finally, we might be able to write a \ACF{ffi}, but they're often brittle and
demanding of our time, due to initial and repeated complex analysis for each
library update. We might even be forced to use particular programming languages.

Often, we look to using mature libraries and frameworks to underpin our
projects, but usually without a guarantee that how we use and connect libraries
is ``safe.'' For example, the sinking of the Vasa ship \cite{wiki:Vasa_ship} was
partially caused by different teams working together but using different
``feet'' units (the Swedish foot is 12'' while the Amsterdam foot is 11'')
resulting in unbalanced weight distribution, contributing to its demise.
Similarly, when the Mars Climate Orbiter travelled to Mars, it met an early
demise due to a navigation issue \cite{Siddiqi2018}. Lockheed Martin built the
orbiter ground controller software, but it didn't conform to \acsp{nasa}
\ACF{sis}. The commands sent from Earth used English units (specifically,
pound-seconds) while the orbiter assumed that it would receive commands using
the metric system (Newton-seconds). As such, the orbiter missed its intended
target orbit altitude, falling into the Martian atmosphere, and ultimately
disintegrating due to atmospheric stress.

Experts had an in-depth understanding of the ``story'' of each project, with
sound rationale for how things worked and should have worked, and yet both
ended in misfortune. Of course, most software is not critical, and issues in
most software will not result in an orbiter disintegrating in Martian atmosphere
or a ship sinking, but there is something that we can learn:
\textit{communication} and \textit{synchronization} of development efforts is
vital for building reliable solutions.

While we as developers don't often build and connect physical things, we do
connect pieces of code\footnote{Though robotics is a field too.}, and
misunderstandings of tacit project knowledge occur too. Thus, we believe we need
to revisit our original sensation when we recognized code duplication and
decided to make reusable components.

The reason is obvious\footnote{Ignoring the more obvious reasons, such as
copy-pasting code, or mere awareness of textual similarities.}! We felt this
sensation because we already had a mental model that connected some key concepts
to some code we wrote, so we decided to make reusable views (code) of our
knowledge. However, the code is only a shallow view of our knowledge, containing
very little discussion of the conceptual underpinnings and the role they play in
the greater ``story.'' Shallow views of implicit, unwritten knowledge,
unfortunately, does not come with guarantee of harmony with the way it's used.

\section{Dreams of Generation}
\label{chap:ideology:sec:thoughts_of_generation}

Unlike the Vasa, for stories where the desired end-product somehow involves
software, we can remedy the communication issue partially by unifying it under
one cohesive story. \Acfp{srs} play a large role in unifying communication of
software needs. However, the communication and maintained synchronization of the
software requirements into the final software product is still brittle (as
evidenced by the Mars orbiter), as it remains heavily reliant on manual labour
to translate it into software artifacts. In other words, the translation from
our knowledge (the important part) is laborious and prone to error, and hence,
not simple enough. So, now we wonder: how can we simplify the process?

Our end-goal should be ``assembly-line'' style engineering of software
\cite{well-understood}, free of logical issues. To attain this, we need to have
clear criteria for what it means for our software artifacts to be free of
logical issues. However, to do this, we need to discuss relevant models. For
example, if we're interested in the accuracy of a bank accounts cached balance,
we discuss \textit{the user}, the relevant \textit{transactions}, and then a
\textit{validation algorithm}. Realizing this example in code might have us
retrieve a user's bank transactions, calculate the expected balance, and compare
it with a cached balance. The code only contains one dimension of this
discussion: the actions. It has no understanding of the substance, nor how it
can be similarly used in other scenarios. To audit the code, we analyze the code
ourselves, potentially with extra testing tools to make things quicker, but in
general, it relies on us and our understanding. To remedy this, we look to
describing software as we've done here: as a ``view'' of some story or
discussion. In other words, we want to build software artifacts through
description as opposed to manual conversion of description to artifact.

\subsection{The Goal}
\label{chap:ideology:sec:thoughts_of_generation:subsec:the_goal}

If we think of a Java compiler as a sort of \textit{generator} of \acs{jvm}
bytecode, we can think of the Java programs as the instructions and inputs to
the generator. We rarely look at the actual bytecode ourselves, but we do have
confidence in knowing what it will do when executed. Now, we look to go one
``level'' up. In other words, we look to inputting our understanding of
particular applications through another generator (an abstraction level up) to
somehow obtain code for it.

Especially in the cases where everything is ``well-understood''
\cite{well-understood}\footnote{Irrelevant of rarity!}, we want to focus on
communicating problems and how solutions solve them so that we can
\textit{generate} usable solutions (a software artifact), and keep them
up-to-date through re-generation.

\subsection{Reconciliation}
\label{chap:ideology:sec:thoughts_of_generation:subsec:reconciliation}

By focusing on capturing well-understood \cite{well-understood} knowledge, we
can use (and re-use) knowledge across specialized generators to generate
software for specific kinds of problems. For example, statisticians frequently
use and discuss various kinds of distributions, such as ``Poisson,''
``Uniform,'' and ``Normal,'' and when they do, they're typically familiar with
their parameters, expectations functions, and how to use them to estimate
likelihoods. Hence, by focusing on using \textit{\acsp{dsl}}, we can build
specialized interpreters for them. Furthermore, by connecting them in precise
manners (with similar precise languages), we can form large meaningful
\textit{networks of domains} \cite{Czarnecki2005} that form our well-understood
problem spaces. For well-understood problem spaces, we can compose a series of
domain-specific interpreters. With enough effort, we could take a whole
``problem description'' that draws in multiple fields, and generate software
that somehow ``solves'' it.

By switching our focus of manual software development to manual problem
description and relationships to ``solutions,'' we shift where we can make bugs,
and how they propagate. Namely, logical bugs will occur more than once so as
long as the same knowledge pulled from is drawn more than once. Thus, each
logical bug should be more visible and easier to spot. Additionally, the
generated software becomes directly \textit{traceable} to its logical
foundations. Hopefully, with adequate dissection of related concepts, bugs
should also be less intricate. Furthermore, generated artifacts are simpler to
\textit{maintain} (i.e., kept in synchronization) with its related
knowledge-base and story by \textit{regenerating} it. Finally, as opposed to
sharing \textit{code}, in this paradigm, we \textit{communicate knowledge},
achieving language-agnostic reusability, focusing on sharing meanings and
families of problems rather than solutions. Thus, by focusing on generating from
meaningful descriptions, we obtain \textit{knowledge reusability} (as opposed to
\textit{code} reusability), and increased software \textit{maintainability},
\textit{reliability}, and \textit{traceability}. Of course, generating all
software artifacts appears grandiose, and, perhaps, reductive or ignorant of
many difficulties in software development. Thus, we must discuss
feasibility\footnote{You may skip the remainder of this chapter if you so wish,
it is not strictly required to understand the rest of my work, but it doesn't
hurt.}.

\subsection{Feasibility}
\label{chap:ideology:sec:thoughts_of_generation:subsec:feasibility}

For us to discuss feasibility of this idealized development paradigm, we must
discuss the \textit{depth} and \textit{breadth} of knowledge we need to make
this feasible\footnote{Note that ``knowledge'' is captured through codifying
\acsp{dsl}.}. Depth of knowledge refers to the vertical knowledge understood
about a specific fragment of knowledge, and its preciseness. For example, we may
have a low-depth of knowledge and claim that English sentences are a sequence of
characters. Alternatively, we might have a slightly ``deeper''
depth/understanding  of sentences by describing them as a language that follows
a specific syntax rule set and using a specific set of words. Breadth of
knowledge is the horizontal domain of knowledge, it is the various kinds of
knowledge we have in a wide variety of subjects and domains.

\subsubsection{Low Depth \& Narrow Breadth}
\label{chap:ideology:sec:thoughts_of_generation:subsec:feasibility:subsubsec:low}

At a shallow depth and narrow breadth of knowledge capture, this paradigm is
very practical, and already heavily used. Widely used \ACFP{cms}, such as
WordPress \cite{WordPress} and Drupal \cite{Drupal}, and web frameworks, such as
Django \cite{Django} and Laravel \cite{Laravel}, are arguably also following
similar ideals as this ideology. Notably, they \textit{deeply embed}
\cite{Carette2009} knowledge in their frameworks and libraries using their host
programming languages basic features. They all typically provide a basic
understanding of ``user's'' of a hosted website, facilities to write \acs{html}
content in one way or another (e.g., \acs{wysiwyg} editors, templates, and
plugins). While some might be, these listed above are not specific to one
specific use-case. They're versatile products, usable for a wide variety of
use-cases because they ship with low but sufficient depth of
knowledge\footnote{They might call it ``features.''} such that you can use them
for a wide variety of different applications (e.g., blogging, ticketing,
booking, accounting, etc.). Out of the box, these web technologies listed come
with simple, common, functionality (features) and powerful extensibility through
either plugins or through software extension and usage. With the basic tooling
provided, users are able to rapidly deploy websites with content. Through
extending the website's knowledge-base (e.g., plugins or software extension),
they are able to obtain a wider breadth and deeper depth to the knowledge
contained within them. Through this, end-users may encode increasingly complex
and different kinds of data into the systems to ultimately obtain increasingly
specialized websites, such as technical blogs, eCommerce websites, online
accounting software, online discussion forums, and more.

The mechanized generation-related components of the ideology is also fairly
shallow in this area, but still, highly feasible. In some sense, almost any
individual instance of ``generation'' is an area of low depth and breadth as
well.

At the lowest depth and narrowest breadth of knowledge capture, we aren't really
capturing any meaning. Rather, we are programming our software artifacts
directly. Hence, this area is already very feasible, evidenced by the usefulness
of software in usage today, and the way said software was made.

\subsubsection{Specialized Tools: Deep Depth \& Narrow Breadth}
\label{chap:ideology:sec:thoughts_of_generation:subsec:feasibility:subsubsec:specialization}

We might think of software that captures deep knowledge about a specific topic
as \textit{specialized tools}. Tailored to specific needs, they come with extra
features for highly specific, potentially niche, categories of
problems\footnote{Think of this category as sharp tools, such as kitchen knives.
We can generally get away with using basic kitchen knives, but specialty knives
help us out for particular use-cases, such as serrated  or filleting
knives.}. These already exist, and are similar to the ``Bottom,'' highly
practical. For example, \ACFP{aml} (such as HashedExpression
\cite{HashedExpression}) allow you to describe complex mathematical algorithms
and generate optimized software tools for solving them. Another example is
parser generators (such as Happy \cite{Happy}). They capture complex information
about grammars and allow users to generate specialized parsers for them. When
needed, specialty tools provide greater functionality than general-purpose
programming languages for solving specific problems.

\subsubsection{Off-the-shelf Solutions: Low Depth \& Wide Breadth}
\label{chap:ideology:sec:thoughts_of_generation:subsec:feasibility:subsubsec:modelling}

Orthogonal to the ``specialized tools,'' a capture of low depth and wide breadth
of knowledge is seemingly a jack of all trades, master of none. For example,
most modern programming languages come with a standard library that provide many
off-the-shelf solutions to well-understood problems, but sticking to very common
generic problems. Most users typically pull in a library that provides extra
specialization for certain problems because the standard library didn't go deep
enough for their needs. Similar to the categories before, these are widely used
and similarly practical.

\subsubsection{Utopia: Deep Depth \& Wide Breadth}
\label{chap:ideology:sec:thoughts_of_generation:subsec:feasibility:subsubsec:utopia}

Finally, we've reached the category of ``deep depth and wide breadth'' of
knowledge capture. Here, we capture the meanings of key ideas and concepts we
need to build software, making as many conscious decisions explicit and visible
as possible. Imagine using the specialized tools to develop every aspect of some
software artifact, but for every facet of the artifact, from start to finish.
This is an idealized method of developing software, where knowledge is strongly
reusable and composable. Of course, this relies on heavy research on tooling. As
long as developers have infinite patience and can invest infinite time into
transcribing and researching to fill in gaps, anything is possible, and this
ideology is very practical! Unfortunately, that situation is not quite
realistic. As such, we should restrict our scope of captured knowledge to
``well-understood'' domains \cite{well-understood}. For areas of
``well-understood'' knowledge, this should be more feasible, merely because the
discussion of key ideas is already coherent and sufficiently codified.

Here, you might use a series of \acsp{dsl} to create a coherent discussion (a
``story''). With it, you would have specialized interpreters built, which
process it, and draw out meaningful byproducts you're interested in (such as
usable software). This story can be relatively far removed from produced
artifacts generating (containing potentially little to no discussion of the
desired artifacts at all), or a precise description (where you might be manually
writing out the final artifacts yourself). Through creating a story as a
composition of many other smaller fragments of knowledge and stories, and
defining interpreters as compositions of other, smaller interpreters, this is
feasible. Hopefully, with enough effort, this should alleviate considerable
stress associated with manual software development, by moving the focus to the
important bits: the story the artifacts tell. The ``quality'' of the generated
artifacts become a traceable reflection of the ``quality'' (depth and breadth)
of the captured knowledge.

\section{A Prospective Workflow}
\label{chap:ideology:sec:a_prospective_workflow}

In theory, all ``development'' should occur in a \ACF{kms}, where knowledge and
transformations between fragments of knowledge are transcribed. ``Users'' would
use a system of pre-filled knowledge to piece together a template story that
fits their narrative, or they would adapt an existing one to fit theirs.

Ideally, the workflow associated with building some product artifact will have
each knowledge/product owner (e.g., actual property ``\textit{owner}'',
developers, managers, designers, etc.) work on strictly the components that are
related to them, and nothing else. At the ``bottom'', the final end-user is
tasked merely with providing feedback that can improve the quality of the
artifact(s). They are the ones that have an issue that can be resolved with some
sort of software artifact. At the ``top,'' product owners designate a basic set
of requirements of the artifacts using a coherent \textit{formal description}.
Product designers/orchestrators will take the requirements and convert them into
a coherent \textit{story} for how the requirements may be translated into a
final product. The story builds on well-understood knowledge of various domains
encoded by domain experts. The product designer is tasked primarily with
translation, while the domain developers and product owners are tasked with
encoding knowledge and instances of knowledge, respectively.

Through product owners describing their requirements coherently (e.g., via some
formal language), completely non-technical product owners may, and will, still
be key figures in the production of the product.

As the stress load/burden becomes shared under this paradigm, the sum of the
parts should be less than the whole. In other words, the cumulative stress of
associated with creating the whole is greater than the approximate sum of each
individual's stress associated with focusing on their respective domain.

Following this ideology, there will be at least 3 key roles associated with
developing artifacts: the \textbf{knowledge encoder}, the \textbf{knowledge
user}, and the \textbf{end-user} of the produced software artifact.

\subsection{Knowledge Encoder (Domain Expert)}
\label{chap:ideology:sec:a_prospective_workflow:subsec:knowledge_encoder}

The knowledge encoder should be a master of a particular domain. They are
expected to encode the knowledge discussed in their respective domain in such a
way that is accessible to those without knowledge of their domain. Additionally,
they should encode information about the ways in which the knowledge can be
transformed into other forms of knowledge (including that which is
interdisciplinary). The knowledge they would be encoding should be as well-known
and globally standardized as possible. As discussed in
\Cref{chap:ideology:sec:thoughts_of_generation:subsec:reconciliation}, it is
likely that the knowledge encoder will focus on writing a series of highly
specific \acsp{dsl}, where the languages may be restricted to as specific as one
term or a handful.

As a domain expert transcribing knowledge encodings of some well-understood
domain, one will largely be discussing the ways in which pieces of knowledge are
\textit{constructed} and \textit{relate to each other}. For the abstract
knowledge encodings to be \textit{usable} in some way, it is vital to have
``names'' (\textit{types}) for the knowledge encodings. In working to capture
the working knowledge of a domain, it's of utmost importance to ensure that all
``instances'' of your ``names'' (types) are \textit{always} usable in some
meaningful way and that the knowledge is exposed in a usable way (e.g.,
sufficiently through some sort of \acs{api}). In other words, all knowledge
encodings should create a stringent, explicit set of rules for which all
``instances'' should conform to, and, arguably, also creates a justification for
the need to create that particular knowledge/data type. As such, optimally, a
domain expert would write their knowledge encodings and renderers in a general
purpose programming language with a sound type system (e.g., Haskell
\cite{Haskell2010}, Agda \cite{Norell2007}, etc.) \textemdash{} preferring ones
with a type system based on formal type theories for their feature richness.

\subsection{Knowledge / Domain User \& Orchestrator}
\label{chap:ideology:sec:a_prospective_workflow:subsec:knowledge_orchestrator}

The knowledge user/orchestrator is tasked with connecting the work of the domain
experts into ``plug-n-play'' stories (arguably, compilers for the end-users to
use). They should also have a working understanding of what the end-user needs,
and how the needs relate to domains of knowledge. As such, they should be able
to encode and reduce friction between knowledge encoded by domain experts and
the goals of product owners.

\subsection{End-user}
\label{chap:ideology:sec:a_prospective_workflow:subsec:end_user}

The final end-user should find the most delight from this ideology. They are the
actual users of the software artifacts, perhaps tweaking the final build of the
software artifacts to be accustomed to their workflow. If the tasks assigned to
the knowledge encoders and the knowledge users are performed correctly, then the
end-user should have strong confidence in the artifacts as they were built with
strict adherence to the knowledge captured at \textit{every step of the way}. As
such, one should confidently expect the final software artifacts to be
completely devoid of unexpected things (including errors, unconformities to
specifications, etc.).

Any of the three (3) user types may use the ``plug-n-play'' stories to describe
problems and generate solutions based on them.

\section{Drasil}
\label{chap:ideology:sec:drasil}

To my understanding, Drasil \cite{Drasil2021} explores this ideology, focusing
on generating scientific software from user-described scientific problems using
Drasil-understood terminology (i.e., ones that a scientific domain expert
previously encoded).
