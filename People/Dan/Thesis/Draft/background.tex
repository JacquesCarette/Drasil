\chapter{Background}
\label{c:background}
\section{Software Artifacts}
\label{sec:sfs}

Software artifacts (or \sfs{}) come in a wide variety of forms and have existed 
since the first programs were created. In the broadest sense, we can think of 
\sfs{} as anything produced during the creation of a piece of software that 
serves some purpose. Any document detailing what the software should do, how it 
was designed, how it was implemented, how to test it, and so on would be 
considered a \sf{}, as would the source code whether as a text file, stack of 
punched cards, magnetic tapes, or other media.

\SFS{} beyond just the source code are important to us for a number of reasons. 
They serve as a means of communication between different stakeholders 
involved in the software development process, or even different generations of 
developers involved in the production of a piece of software. They provide a 
common understanding of what the software is supposed to do, how it will be 
built, and how it will be tested. \SFS{} also provide a record of the decisions 
that were made during the development process which is important for future 
maintenance/modification of the software or for compliance reasons. Combined 
with the former, this gives us a means to verify and validate the software 
against its original requirements and design. The production, maintenance, and 
use of \sfs{} is largely dependent on the specific design process being used 
within a team.

\fig{
\begin{center}
\includegraphics[width=\linewidth]{figures/Waterfall.png}
\end{center}}
{The Waterfall Model of Software Development}
{fig:Waterfall}


\begin{table}[h!]
\caption{Mapping of Rational Design Process to Waterfall Model and Common
Software Artifacts}
\label{tab:RatWatComp}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|
  >{\raggedright\arraybackslash}p{0.28\linewidth}|
  >{\raggedright\arraybackslash}p{0.28\linewidth}|
  >{\raggedright\arraybackslash}p{0.36\linewidth}|
}
\hline
\textbf{Rational Design Phase} & \textbf{Waterfall Phase(s)} & \textbf{Common
Artifacts} \\
\hline
Establish/Document Requirements & Requirements Analysis &
System Requirements Specification; Verification and Validation Plan \\
\hline
Design and Document Module Structure & System Design &
Design Document \\
\hline
Design and Document Module Interfaces & Program Design &
Module Interface Specification \\
\hline
Design and Document Module Internal Structures & Program Design &
Module Guide \\
\hline
Write Programs & Coding &
Source Code; Build Instructions \\
\hline
Maintain & 
Testing (Unit, Integration, System, Acceptance) \newline\newline Operation \& 
Maintenance &
Test Cases; Verification and Validation Report \\
\hline
\end{tabular}
\end{table}

Software design can follow a number of different design processes, each with 
their own collection of \sfs{}. A common, traditional approach is the Waterfall 
model (Figure~\ref{fig:Waterfall}) of software 
development~\cite{PfleegerAndAtlee2010Ch2}. 
However, Parnas and Clements~\cite{ParnasAndClements1986} detailed what they 
dubbed a ``rational" design process; an idealized version of software 
development which includes what needs to be documented in corresponding \sfs{}.
The rational design process is not meant to be a linear process like the 
Waterfall model, but instead an iterative process using section stubs for 
information that is not yet available or not fully clear during the time of 
writing. Those stubs are then filled in over the development process, and 
existing documentation is updated so it appears to have been created in a 
linear fashion for ease of review later in the software lifecycle. The rational 
design process involves the steps listed in the first column of 
Table~\ref{tab:RatWatComp}.

Parnas provided a list of the most important documents~\cite{Parnas2010} 
required for the rational design process, which 
Smith~\cite{SmithAndKoothoor2016} expanded upon by including complimentary 
artifacts such as source code, verification and validation plans, test cases, 
and build instructions. While there have been many proposed artifacts, the 
following collection covers those most relevant to this thesis:

\cardifact{Software Requirements Specification}{Contains the functional and 
nonfunctional requirements detailing what the desired software system should 
do.} 
\cardifact{System Design Document}{Explains how the system should be broken 
down and documents implementation-level decisions that have been made for the 
design of the system.} 
\cardifact{Module Guide}{In-depth explanation of the modules outlined in the 
System Design Document.}
\cardifact{Module Interface Specification}{Interface specification for each of 
the modules outlined in the System Design Document/Module Guide.} 
\cardifact{Program Source Code}{The source code of the implemented software 
system.} 
\cardifact{Verification and Validation Plan}{Uses the system requirements to 
document acceptance criteria for each requirement that can be validated.}
\cardifact{Test cases}{Implementation of the Verification and Validation Plan 
in source code (where applicable) or as a step-by-step guide for testers.} 
\cardifact{Verification and Validation Report}{Report outlining the results 
after undertaking all of the testing initiatives outlined in the Verification 
and Validation plan and test cases.}

This collection is not exhaustive of all the types of \sfs{}, as there are many 
other design processes which use different types of \sfs{}. Looking at an agile 
approach using Scrum/Kanban, the \sfs{} tend to be distributed in different 
ways~\cite{KasauliEtAl2017}. Requirements are documented in tickets under 
so-called ``epics", ``stories", and ``tasks" as opposed to a singular 
requirements artifact, and the acceptance criteria listed on those tickets make 
up the validation and verification plan. 

Regardless of the process used, most attempt to document very similar 
information to that of the rational design process. Using the waterfall model 
as an example, we can see (Table~\ref{tab:RatWatComp}) the rational design 
process and its artifacts map onto the model in a very straightforward manner.

As mentioned earlier, \sfs{} are important to development in a number of ways, 
such as easing the burden of maintenance and training. We outline the artifacts 
we are most interested below with a brief description of their purpose.

Parnas~\citep{Parnas2010} does an excellent job of defining the target audience 
for each of the most common \sfs{} and we extend that alongside our work. A 
summary can be found in Table~\ref{tab:sfsummary}.

\begin{table}[h!]
\caption{A summary of the Audience for each of the most common \sfs{} and what 
problem that \sf{} is solving}
\label{tab:sfsummary}
\begingroup
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|
  >{\raggedright\arraybackslash}p{.2\linewidth} |
  >{\raggedright\arraybackslash}p{.2\linewidth} |
  >{\raggedright\arraybackslash}p{.45\linewidth} |
}
\hline
 \textbf{\SF{}} & \textbf{Who (Audience)} & \textbf{What (Problem/Solution)}
\\ \hline
    SRS & 

    Software Architects \newline \newline
    QA analysts & 

    Defines exactly what specification the software system must adhere to. 
\\ \hline
    Module Guide & 

    All developers \newline \newline
    QA analysts  & 

    Outlines implementation decisions around the separation of functionality 
    into modules and give an overview of each module.
\\ \hline
    Module Interface Specifications & 

    Developers who implement the module \newline \newline
    Developers who use the module \newline \newline
    QA analysts & 

    Details the exact interfaces of the modules from the Module Guide.
\\ \hline
    Source Code / Executable &

    All developers \newline \newline
    QA analysts \newline \newline 
    End users &

    Implements the machine instructions designed to address the overall problem 
    for which the software system has been specified
\\ \hline
    Verification and Validation Plan &

    Developers who implement the module \newline \newline
    QA analysts &

    Describes how the software should be verified using tests that can be 
    validated. Includes module-specific and system-wide plans.
\\ \hline
\end{tabular}
\endgroup
\end{table}

\section{Software Reuse and Software Families}

In this section we look at ways in which others have attempted to avoid 
``reinventing the wheel" by providing means to reuse software, in part or 
whole, and reuse the analysis and design efforts of those that came before.

\subsection{Software/Program Families}

Software/program families refer to a group of related systems sharing a common 
set of features, functionality, and design~\cite{Pohl2005}. These systems are 
typically designed to serve a specific domain or market, and are often 
developed using a common set of core technologies and design principles.

One well-known example of a software family is the Microsoft Office suite. Each 
program in the suite is used for a specific application (word 
processing, spreadsheet management, presentation tools, etc), yet they all have 
similar design principles and user interface features. A user who understands 
how to use one of the software family members will have an intuitive sense of 
how to use the others thanks to the common design features. 

Another, much larger scale, software family is that of the GNU/Linux operating 
system (OS) and its various distributions\cite{AttarianEtAl2013}. There are 
many variations on the OS depending on the user's needs. For an everyday 
computer desktop experience, there are general purpose distributions (Ubuntu, 
Linux Mint, Fedora, Red Hat, etc.). For server/data center applications, there
are specialized server distributions (Ubuntu Server, Fedora Server, Debian, 
etc.). For systems running on embedded hardware there are lightweight, 
specialized, embedded distributions built for specific architecture (Armbian, 
RaspbianOS, RedSleeve, etc.). There are even specialized distributions that are 
meant to be run without persistent storage for specific applications like 
penetration/network testing (Kali Linux, Backbox, Parrot Security OS, 
BlackArch, etc.). However, if you are familiar with one particular flavour of 
linux, you'll likely be comfortable moving between several of the distributions
built upon the same cores. You may even be familiar moving to other *NIX based 
systems like MacOS/Unix.

Software/program families can provide a range of benefits to both developers 
and end-users. For developers, software families can increase 
productivity by providing a reusable set of core technologies and design 
principles~\cite{Pohl2005}. This can help reduce the time and effort required 
to develop new systems, and improve the quality and consistency of the 
resulting software. For end-users, program families provide a range of 
usability and functional benefits. Common features and UI elements improve the 
user experience by making it easier for users to learn and use multiple 
systems~\cite{Bosch2000}. By also providing a range of related 
applications, such as those provided by the Microsoft office suite, software 
families help meet users' needs across a wider range of domains.

\subsection{Software Reuse}

Reusing software is an ideal means of reducing costs. If we can avoid spending 
time developing new software and instead use some existing application (or a 
part therein), we save time and money. There have been many proposals on ways 
to encourage software reuse, each with their own merits and drawbacks.

Component based software engineering (CBSE) is one such example. CBSE is an 
approach to software development that emphasizes using pre-built software 
components as the building blocks of larger systems. These reusable components 
can be developed independently and tested in isolation before being integrated 
into the larger system software.

One key benefit of CBSE, as mentioned above, is that it can help to reduce the 
cost and time required for software development, since developers do not need 
to implement everything from scratch. Additionally, CBSE can improve the 
quality and reliability of software systems, since components have typically 
been thoroughly tested and previously used in other contexts.

One CBSE framework is the Common Object Request Broker Architecture (CORBA), 
which provides a standardized mechanism for components to 
communicate with each other across a network. CORBA defines a set of interfaces 
and protocols that allow components written in different programming languages 
to interact with each other in a distributed environment~\cite{OMG2000}.

Another CBSE framework is the JavaBeans Component Architecture. It is a 
standard for creating reusable software components in Java. JavaBeans are 
self-contained software modules with a well-defined interface that can be 
easily integrated into a variety of development environments and combined to 
form larger applications~\cite{OracleJavaBeans}.

The largest challenge of CBSE is ensuring components are compatible with others 
and can be integrated into a larger system without conflicts or errors. In an 
effort to address this challenge, numerous approaches to component 
compatibility testing have been proposed~\cite{Wu2001}.

Neighbors~\cite{Neighbors1980,Neighbors1984,Neighbors1989} proposed a method
for engineering reusable software systems known as ``Draco''. Draco was among
the earliest frameworks for software reuse, particularly in the context of
domain-specific software generation, though its direct influence was more
limited compared to later, widely adopted approaches. The core idea behind
Draco is to construct software systems by composing reusable, domain-specific
components called ``domain knowledge modules''. In Draco, a software engineer
first defines a domain model, capturing the essential abstractions and
operations relevant to a particular problem area. The system then provides a
set of reusable transformations and code generation templates tailored to that
domain. By selecting and configuring these modules, developers can
automatically generate significant portions of a software system, reducing
manual coding effort. While Draco itself is no longer widely used, its
approach anticipated later developments in software product lines and
generative programming by emphasizing the explicit capture and reuse of domain
knowledge~\cite{Neighbors1984,Czarnecki2000}. Draco's concepts have informed
subsequent research in generative programming and domain-specific language
design, even if its practical adoption remained limited.

\subsection{Reproducible Research}

Being able to reproduce results, is fundamental to the idea of good science.
When it comes to software projects, there are often many undocumented
assumptions or modifications (including hacks) involved in the finished product.
This can make replication impossible without the help of the original author,
and in some cases reveal errors in the original author's
work~\cite{IonescuAndJansson2013}.

Reproducible research has been used to mean embedding executable code in
research papers to allow readers to reproduce the results
described~\cite{SchulteEtAl2012}.

Combining research reports with relevant code, data, etc.\ is not necessarily
easy, especially when dealing with the publication versions of an author's work.
As such, the idea of \emph{compendia} were
introduced~\cite{GentlemanAndLang2012} to provide a means of encapsulating the
full scope of the work. Compendia allow readers to see computational details, as
well as re-run computations performed by the author. Gentleman and Lang proposed
that compendia should be used for peer review and distribution of scientific
work~\cite{GentlemanAndLang2012}.

Currently, several tools have been developed for reproducible research
including, but not limited to, Sweave~\cite{Leisch2002},
SASweave~\cite{LenthEtAl2007}, Statweave~\cite{Lenth2009},
Scribble~\cite{FlattEtAl2009}, and Org-mode~\cite{SchulteEtAl2012}. The most
popular of those being Sweave~\cite{SchulteEtAl2012}. The aforementioned tools
maintain a focus on code and certain computational details. Sweave,
specifically, allows for embedding code into a document which is run as the
document is being typeset so that up to date results are always included.
However, Sweave (along with many other tools), still maintains a focus on
producing a single, linear document. 
%\ds{this doesn't belong here}
%It is my hope that Drasil will outperform
%these existing tools due to its flexibility and its ability to create multiple
%artifacts from a knowledge base.

\section{Literate Approaches to Software Development}

There have been several approaches attempting to combine development of program 
code with documentation. Literate Programming and literate software are two 
such approaches that have influenced the direction of this thesis. Each of 
these approaches is outlined in the following sections.

\subsection{Literate Programming}

Literate Programming (LP) is a method for writing software introduced by Knuth 
that focuses on explaining to a human what we want a computer to do rather than 
simply writing a set of instructions for the computer on how to perform the 
task~\cite{Knuth1984}.

Developing literate programs involves breaking algorithms down into
\emph{chunks}~\cite{JohnsonAndJohnson1997} or \emph{sections}~\cite{Knuth1984}
which are small and easily understandable. The chunks are ordered to follow a 
``psychological order''~\cite{PieterseKourieAndBoake2004} if
you will, that promotes understanding. They do not have to be written in the 
same order that a computer would read them. It should also be noted that in a 
literate program, the code and documentation are kept together in one source. 
To extract runnable code, a process known as \emph{tangle} must be performed on 
the source. A similar process known as \emph{weave} is used to extract and 
typeset the documentation.

There are many advantages to LP beyond understandability. As a program is
developed and updated, the documentation surrounding the source code is more 
likely to be updated simultaneously. It has been experimentally found that 
using LP ends up with more consistent documentation and 
code~\cite{ShumAndCook1993}. There are many downsides to having inconsistent 
documentation while developing or maintaining 
code~\cite{Kotula2000,Thimbleby1986}, while the benefits of consistent 
documentation are numerous~\cite{Hyman1990, Kotula2000}. Keeping the advantages 
and disadvantages of good documentation in mind we can see that more effective, 
maintainable code can be produced if properly using 
LP~\cite{PieterseKourieAndBoake2004}.

Regardless of the benefits of LP, it has not been very popular with 
developers~\cite{ShumAndCook1993}. However, there are
several successful examples of LP's use in SC. Two such literate programs that 
come to mind are VNODE-LP~\cite{Nedialkov2006} and ``Physically Based 
Rendering: From Theory to Implementation''~\cite{PharrAndHumphreys2004} a 
literate program and textbook on the subject matter. Shum and 
Cook~\cite{ShumAndCook1993} discuss the main issues behind LP's lack of 
popularity which can be summed up as dependency on a 
particular output language or text processor, and the lack of flexibility on 
what should be presented or suppressed in the output.

There are several other factors which contribute to LP's lack of popularity and 
slow adoption thus far. While LP allows a developer to write their code and its 
documentation simultaneously, that documentation is comprised of a single 
artifact which may not cover the same material as standard artifacts software 
engineers expect (see Section~\ref{sec:sfs} for more details). LP also does not 
simplify the development process: documentation and code are written as usual, 
and there is the additional effort of re-ordering the chunks. The LP 
development process has some benefits such as allowing developers to follow a 
more natural flow in development by writing chunks in whichever order they 
wish, keep the documentation and code updated simultaneously (in theory) 
because of their co-location, and automatically incorporate code chunks into 
the documentation to reduce some information duplication.

There have been many attempts to increase LP's popularity by focusing on 
changing the output language or removing the text processor dependency. Several
new tools such as CWeb (for the C language), DOC++ (for C++), noweb 
(programming language independent), and others were developed. Other tools such 
as javadoc (for Java) and Doxygen (for multiple languages) were also influenced 
by LP, but differ in that they are merely document extraction tools. They do 
not contain the chunking features which allow for re-ordering algorithms.

With new tools came new features including, but not limited to, phantom
abstracting~\cite{ShumAndCook1993}, a ``What You See Is What You Get'' (WYSIWYG)
editor~\cite{FritzsonGunnarssonAndJirstrand2002}, and even movement away from 
the ``one source'' idea~\cite{Simonis2003}.

While LP is still not mainstream~\cite{Ramsey1994}, these more robust 
tools helped drive the understanding behind what exactly LP tools must 
do. In certain domains LP is becoming more standardized, for 
example: Agda, Haskell, and R support LP to some extent, even though it is not 
yet common practice. R has good tool support, with the most popular being
Sweave~\cite{Leisch2002}, however it is designed to dynamically create
up-to-date reports or manuals by running embedded code as opposed to being used
as part of the software development process. 

\subsection{Literate Software}

A combination of LP and Box Structure~\cite{Mills1986} was proposed as a new
method called ``Literate Software Development''
(LSD)~\cite{AlMatiiAndBoujarwah2002}. Box structure can be summarized as the
idea of different views which are abstractions that communicate the same
information in different levels of detail, for different purposes. Box
structures consist of black box, state machine, and clear box structures. The
black box gives an external (user) view of the system and consists of stimuli
and responses; the state machine makes the state data of the system visible (it
defines the data stored between stimuli); and the clear box gives an internal
(designer's) view describing how data are processed, typically referring to
smaller black boxes~\cite{Mills1986}. These three structures can be nested as
many times as necessary to describe a system.

LSD was developed with the intent to overcome the disadvantages of both LP and
box structure. It was intended to overcome LP's inability to specify interfaces
between modules, the inability to decompose boxes and implement the design
created by box structures, as well as the lack of tools to support box
structure~\cite{Deck1996}.

The framework developed for LSD, ``WebBox'', expanded LP and box structures in a
variety of ways. It included new chunk types, the ability to refine chunks, the
ability to specify interfaces and communication between boxes, and the ability
to decompose boxes at any level. However, literate software (and LSD) remains
primarily code-focused with very little support for creating other software
artifacts, in much the same way as LP.

\section{Generative Programming}

Generative programming is an approach to software development that focuses on 
automating the process of generating code from high-level 
specifications~\cite{Czarnecki2000,Taha2006}. By writing a program 
specification and feeding it to the generator, one does not have to manually 
implement the desired program.

One of the primary benefits of generative programming is that it can help to 
increase productivity and reduce the time and effort required to develop 
software~\cite{Czarnecki2000}. By automating the generation of code, developers 
can focus on high-level design and specification, rather than low-level 
implementation details.

Generative programming has the added benefit of helping to improve the quality 
of software by reducing the risk of errors and 
inconsistencies~\cite{Czarnecki2000,Taha2006}. Since the code is generated 
automatically from high-level specifications, there is less room for human 
error, and the generated code is typically more consistent and predictable.

There are also some potential drawbacks to generative programming. For 
instance, the generated code may not always be optimal or 
efficient~\cite{Czarnecki2000,Taha2006}.As the code is generated 
automatically, it may not take into account all of the nuance or complexity
of the underlying system potentially leading to suboptimal performance or other 
issues.

Generative programming also requires a significant upfront investment in time 
and effort to develop the generators and other tools needed to automate the 
process of code generation~\cite{Czarnecki2000,Taha2006}. This means it is 
often not worth the effort to use generative programming for one-off projects.

There are a large number of generative programming tools available today. Some, 
like template metaprogramming (TMP) tools, are built into a number of 
programming languages (C++, Rust, Scala, Java, Go, Python, etc.) and offer 
varying levels of support for generative programming. Others are widely used 
across a multitude of disparate domains. For example, \textbf{ANTLR} (Another 
Tool for Language Recognition) automatically generates parsers and lexers from 
grammar specifications, enabling rapid development of compilers and 
interpreters~\cite{Parr1995}. \textbf{YACC} (Yet Another Compiler Compiler) and 
its derivatives serve a similar purpose for C-based 
toolchains~\cite{Johnson1975}. In web development, frameworks like 
\textbf{Yeoman} scaffold entire web applications by generating boilerplate 
code, configuration files, and project structure~\cite{Baumgartner2016}. 
\textbf{Swagger Codegen} and the \textbf{OpenAPI Generator} produce client 
libraries, server stubs, and API documentation from OpenAPI specifications, 
streamlining the development of RESTful services~\cite{openapi2025, 
swagger2025}. In embedded systems, tools such as \textbf{Simulink Coder} 
generate C/C++ code from graphical models, facilitating deployment on hardware 
targets~\cite{Chaturvedi2017}. These examples illustrate the breadth of 
generative programming, from language processing to web and embedded systems, 
and demonstrate how code generators can accelerate development and improve 
consistency across software projects.