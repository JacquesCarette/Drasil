\chapter{Background}
\label{c:background}

This chapter surveys foundational concepts and approaches that underpin the 
thesis. Each section is selected to clarify the context, motivations, and 
limitations of the work. We structure the review as follows:

\begin{itemize}
  \item First we introduce literature around software artifacts, emphasizing 
  	their role in communication, maintenance, and verification within 
  	software engineering.
  \item Next we examine software reuse, software families, and reproducible 
	research, highlighting strategies for leveraging prior work and domain 
	knowledge.
  \item Then we discuss literate approaches to software development to motivate
    the integration of documentation and code.
  \item Finally, we review Generative programming as a means of automating 
  	artifact creation from high-level specifications.
\end{itemize}

We have selected specific topic areas following a criterion for inclusion 
regarding the direct relevance to our central problem: improving the 
production and maintenance of software artifacts through automation and 
knowledge reuse. Topics are selected if they inform the design, implementation, 
or evaluation of the proposed approach. Areas such as Model-Driven Engineering 
(MDE), or Knowledge-Based Software Engineering (KBSE)\footnote{While KBSE 
shares some conceptual overlap, its primary concerns (such as expert systems, 
rule-based reasoning, and formal methods) do not directly inform the design, 
implementation, or evaluation of Drasil.} while significant in the broader 
literature, are excluded unless they directly intersect with the thesis's 
scope. This ensures the chapter remains focused and purposeful, rather 
than exhaustive.

\section{Software Artifacts}
\label{sec:sfs}

Software artifacts (or \sfs{}) come in a wide variety of forms and have existed 
since the first programs were created. In the broadest sense, we can think of 
\sfs{} as anything and everything produced during the creation of a piece of 
software that serves some purpose. Any document detailing what the software 
should do, how it was designed, how it was implemented, how to test it, and so 
on would be considered a \sf{}, as would the source code whether as a text 
file, stack of punched cards, magnetic tapes, or other media. \SFS{} also 
include items such as build instructions, Continuous Integration/Continuous 
Delivery (CI/CD) pipeline definitions, automated test cases, READMEs, etc.

\SFS{} beyond just the source code are important to us for a number of reasons. 
They serve as a means of communication between different stakeholders 
involved in the software development process, or even different generations of 
developers involved in the production of a piece of software. They provide a 
common understanding of what the software is supposed to do, how it will be 
built, and how it will be tested. \SFS{} also provide a record of the decisions 
that were made during the development process, which is important for future 
maintenance/modification of the software or for compliance reasons. Combined 
with the former, this gives us a means to verify and validate the software 
against its original requirements and design. The production, maintenance, and 
use of \sfs{} is largely dependent on the specific design process being used 
within a team.

\fig{
\begin{center}
\includegraphics[width=\linewidth]{figures/Waterfall.png}
\end{center}}
{The Waterfall Model of Software Development}
{fig:Waterfall}


\begin{table}[h!]
\caption{Mapping of Rational Design Process to Waterfall Model and Common
Software Artifacts}
\label{tab:RatWatComp}
\centering
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|
  >{\raggedright\arraybackslash}p{0.28\linewidth}|
  >{\raggedright\arraybackslash}p{0.28\linewidth}|
  >{\raggedright\arraybackslash}p{0.36\linewidth}|
}
\hline
\textbf{Rational Design Phase} & \textbf{Waterfall Phase(s)} & \textbf{Common
Artifacts} \\
\hline
Establish/Document Requirements & Requirements Analysis &
System Requirements Specification; Verification and Validation Plan \\
\hline
Design and Document Module Structure & System Design &
Design Document \\
\hline
Design and Document Module Interfaces & Program Design &
Module Interface Specification \\
\hline
Design and Document Module Internal Structures & Program Design &
Module Guide \\
\hline
Write Programs & Coding &
Source Code; Build Instructions \\
\hline
Maintain & 
Testing (Unit, Integration, System, Acceptance) \newline\newline Operation \& 
Maintenance &
Test Cases; Verification and Validation Report \\
\hline
\end{tabular}
\end{table}

Software design can follow a number of different design processes, each with 
their own collection of \sfs{}. A common, traditional approach is the Waterfall 
model (Figure~\ref{fig:Waterfall}) of software 
development~\cite{PfleegerAndAtlee2010Ch2}, which often does not work in 
practice~\cite{}. \ds{Need this source.}
However, Parnas and Clements~\cite{ParnasAndClements1986} detailed what they 
dubbed a ``rational" design process; an idealized version of software 
development that includes what needs to be documented in corresponding \sfs{}.
The rational design process is not meant to be a linear process like the 
Waterfall model, but instead an iterative process using section stubs for 
information that is not yet available or not fully clear during the time of 
writing. Those stubs are then filled in over the development process, and 
existing documentation is updated so it appears to have been created in a 
linear fashion for ease of review later in the software lifecycle. The rational 
design process involves the steps listed in the first column of 
Table~\ref{tab:RatWatComp}.

Parnas provided a list of the most important documents~\cite{Parnas2010} 
required for the rational design process, which 
Smith~\cite{Smith2016} expanded upon by including complimentary 
artifacts such as source code, verification and validation plans, test cases, 
and build instructions. While there have been many proposed artifacts, the 
following collection covers those most relevant to this thesis:

\cardifact{Software Requirements Specification}{Contains the functional and 
nonfunctional requirements detailing what the desired software system should 
do.} 
\cardifact{System Design Document}{Explains how the system should be broken 
down and documents implementation-level decisions that have been made for the 
design of the system.} 
\cardifact{Module Guide}{In-depth explanation of the modules outlined in the 
System Design Document.}
\cardifact{Module Interface Specification}{Interface specification for each of 
the modules outlined in the System Design Document/Module Guide.} 
\cardifact{Program Source Code}{The source code of the implemented software 
system.} 
\cardifact{Verification and Validation Plan}{Uses the system requirements to 
document acceptance criteria for each requirement that can be validated.}
\cardifact{Test cases}{Implementation of the Verification and Validation Plan 
in source code (where applicable) or as a step-by-step guide for testers.} 
\cardifact{Verification and Validation Report}{Report outlining the results 
after undertaking all of the testing initiatives outlined in the Verification 
and Validation plan and test cases.}

This collection is not exhaustive of all the types of \sfs{}, as there are many 
other design processes that use different types of \sfs{}. For example, looking 
at an agile approach using Scrum/Kanban, the \sfs{} tend to be distributed in 
different ways~\cite{KasauliEtAl2017}. Requirements are documented in tickets 
under so-called ``epics", ``stories", and ``tasks" as opposed to a singular 
requirements artifact, and the acceptance criteria listed on those tickets make 
up the validation and verification plan. 

Regardless of the process, most attempt to document very similar information to 
that of the rational design process. Using the waterfall model as an example, 
we can see (Table~\ref{tab:RatWatComp}) the rational design process and its 
artifacts map onto the model in a very straightforward manner.

As mentioned earlier, \sfs{} are important to development in a number of ways, 
such as easing the burden of maintenance and training. We outline the artifacts 
we are most interested in below with a brief description of their purpose.

Parnas~\citep{Parnas2010} does an excellent job of defining the target audience 
for each of the most common \sfs{} and we extend that alongside our work. A 
summary can be found in Table~\ref{tab:sfsummary}.

\begin{table}[h!]
\caption{A summary of the Audience for each of the most common \sfs{} and what 
problem that \sf{} is solving}
\label{tab:sfsummary}
\begingroup
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|
  >{\raggedright\arraybackslash}p{.2\linewidth} |
  >{\raggedright\arraybackslash}p{.2\linewidth} |
  >{\raggedright\arraybackslash}p{.45\linewidth} |
}
\hline
 \textbf{\SF{}} & \textbf{Who (Audience)} & \textbf{What (Problem/Solution)}
\\ \hline
    SRS & 

    Software Architects \newline \newline
    QA analysts & 

    Defines exactly what specification the software system must adhere to. 
\\ \hline
    Module Guide & 

    All developers \newline \newline
    QA analysts  & 

    Outlines implementation decisions around the separation of functionality 
    into modules and give an overview of each module.
\\ \hline
    Module Interface Specifications & 

    Developers who implement the module \newline \newline
    Developers who use the module \newline \newline
    QA analysts & 

    Details the exact interfaces of the modules from the Module Guide.
\\ \hline
    Source Code / Executable &

    All developers \newline \newline
    QA analysts \newline \newline 
    End users &

    Implements the machine instructions designed to address the overall problem 
    for which the software system has been specified
\\ \hline
    Verification and Validation Plan &

    Developers who implement the module \newline \newline
    QA analysts &

    Describes how the software should be verified using tests that can be 
    validated. Includes module-specific and system-wide plans.
\\ \hline
\end{tabular}
\endgroup
\end{table}

\section{Software Reuse and Software Families}

In this section we look at ways in which others have attempted to avoid 
``reinventing the wheel" by providing means to reuse software, in part or 
whole, and reuse the analysis and design efforts of those that came before.
We focus on software families (software product lines), component-based 
software engineering (CBSE), frameworks for reuse such as Draco, and the 
concept of reproducible research. These techniques were selected because they 
represent significant strategies for leveraging prior work, domain expertise, 
and automation in software development. Software families and CBSE illustrate 
structural and compositional reuse, Draco demonstrates early domain-specific 
reuse and code generation, while reproducible research highlights the 
importance of transparency and repeatability in computational work. Together, 
these approaches inform our emphasis on automating artifact generation 
and promoting knowledge reuse across the software lifecycle.

\subsection{Software/Program Families}

Software/program families, or software product lines, refer to a group of 
related systems sharing a common set of features, functionality, and 
design~\cite{Pohl2005}. These systems are typically designed to serve a 
specific domain or market, and are often developed using a common set of core 
technologies and design principles.

A central concept in software/program families is the distinction between 
\emph{commonalities} and \emph{variabilities}~\cite{Bosch2000,Pohl2005}. 
\emph{Commonalities} refer to the features, components, or design decisions 
shared by all members of the family, forming the stable core that defines the 
family's identity. In contrast, \emph{variabilities} capture the aspects that 
can differ among family members, enabling customization or adaptation to 
specific requirements or contexts. These variabilities are typically managed 
through explicit \emph{parameters of variation}, which may include 
configuration options, feature selections, or architectural choices. 
Identifying and systematically managing these parameters is essential for 
effective reuse and automation in software product lines, as it allows 
developers to generate tailored systems while maintaining consistency and 
reducing duplication.

One well-known example of a software family is the Microsoft Office suite. Each 
program in the suite is used for a specific application (word 
processing, spreadsheet management, presentation tools, etc), yet they all have 
similar design principles and user interface features. A user who understands 
how to use one of the software family members will have an intuitive sense of 
how to use the others thanks to the common design features. 

Another, much larger scale, software family is that of the GNU/Linux operating 
system (OS) and its various distributions\cite{AttarianEtAl2013}. There are 
many variations on the OS depending on the user's needs. For an everyday 
computer desktop experience, there are general purpose distributions (Ubuntu, 
Linux Mint, Fedora, Red Hat, etc.). For server/data center applications, there
are specialized server distributions (Ubuntu Server, Fedora Server, Debian, 
etc.). For systems running on embedded hardware there are lightweight, 
specialized, embedded distributions built for specific architecture (Armbian, 
RaspbianOS, RedSleeve, etc.). There are even specialized distributions that are 
meant to be run without persistent storage for specific applications like 
penetration/network testing (Kali Linux, Backbox, Parrot Security OS, 
BlackArch, etc.). However, if you are familiar with one particular flavour of 
Linux, you'll likely be comfortable moving between several of the distributions
built upon the same cores. You may even be familiar moving to other *NIX based 
systems like MacOS/Unix.

Software/program families can provide a range of benefits to both developers 
and end-users. For developers, software families can increase 
productivity by providing a reusable set of core technologies and design 
principles~\cite{Pohl2005}. This can help reduce the time and effort required 
to develop new systems, and improve the quality and consistency of the 
resulting software. For end-users, program families provide a range of 
usability and functional benefits. Common features and UI elements improve the 
user experience by making it easier for users to learn and use multiple 
systems~\cite{Bosch2000}. By also providing a range of related 
applications, such as those provided by the Microsoft office suite, software 
families help meet users' needs across a wider range of domains.

\subsection{Software Reuse}

Reusing software is an ideal means of reducing costs. If we can avoid spending 
time developing new software and instead use some existing application (or a 
part therein), we save time and money. There have been many proposals on ways 
to encourage software reuse, each with their own merits and drawbacks.

Component based software engineering (CBSE) is one such example. CBSE is an 
approach to software development that emphasizes using pre-built software 
components as the building blocks of larger systems. These reusable components 
can be developed independently and tested in isolation before being integrated 
into the larger system software.

A key benefit of CBSE is it can help to reduce the cost and time required for 
software development, since developers do not need to implement everything from 
scratch. Additionally, CBSE can improve the quality and reliability of software 
systems, since components have typically been thoroughly tested and previously 
used in other contexts.

One CBSE framework is the Common Object Request Broker Architecture (CORBA), 
which provides a standardized mechanism for components to 
communicate with each other across a network. CORBA defines a set of interfaces 
and protocols that allow components written in different programming languages 
to interact with each other in a distributed environment~\cite{OMG2000}.

Another CBSE framework is the JavaBeans Component Architecture. It is a 
standard for creating reusable software components in Java. JavaBeans are 
self-contained software modules with a well-defined interface that can be 
easily integrated into a variety of development environments and combined to 
form larger applications~\cite{OracleJavaBeans}.

The largest challenge of CBSE is ensuring components are compatible with others 
and can be integrated into a larger system without conflicts or errors. In an 
effort to address this challenge, numerous approaches to component 
compatibility testing have been proposed~\cite{Wu2001}. This limitation motivates our approach to explore reuse beyond mere composition, focusing instead on capturing and reusing domain knowledge to generate consistent, compatible artifacts.

Others have attempted to provide frameworks for reuse as well. For example,
Neighbors~\cite{Neighbors1980,Neighbors1984,Neighbors1989} proposed a method
for engineering reusable software systems known as ``Draco''. Draco was among
the earliest frameworks for software reuse, particularly in the context of
domain-specific software generation, though its direct influence was more
limited compared to later approaches. The core idea behind Draco is to construct software systems by composing reusable, domain-specific components called ``domain knowledge modules''. Unlike CBSE, which focuses on assembling systems from independently developed, interchangeable components, Draco emphasizes the reuse of domain-specific knowledge and automated code generation through configurable transformations.

In Draco, a software engineer first defines a domain model, capturing the 
essential abstractions and operations relevant to a particular problem area. 
The system then provides a set of reusable transformations and code generation 
templates tailored to that domain. By selecting and configuring these modules, 
developers can automatically generate significant portions of a software 
system, reducing manual coding effort. While Draco itself is not widely 
used, its approach anticipated later developments in software product lines and
generative programming by emphasizing the explicit capture and reuse of domain
knowledge~\cite{Neighbors1984,Czarnecki2000}. We draw inspiration from Draco's focus on domain knowledge reuse and automated artifact generation in our own approach.

\subsection{Reproducible Research}

Being able to reproduce results, is fundamental to the idea of good science.
When it comes to software projects, there are often many undocumented
assumptions or modifications (including hacks) involved in the finished product.
This can make replication impossible without the help of the original author,
and in some cases reveal errors in the original author's
work~\cite{IonescuAndJansson2013}.

Reproducible research has been used to mean embedding executable code in
research papers (example: using Jupyter Notebooks~\cite{Jupyter}) to allow 
readers to reproduce the results described~\cite{SchulteEtAl2012}. We have designed Drasil to go beyond this narrow definition of reproducible research by
emphasizing the generation of multiple, consistent artifacts from a single knowledge base. This approach not only facilitates the reproduction of computational results but also ensures that all related documentation, code, and tests remain synchronized and up-to-date, thereby enhancing the overall reproducibility and reliability of scientific software.

Combining research reports with relevant code, data, etc. is not necessarily
easy, especially when dealing with the publication versions of an author's work.
As such, the idea of \emph{compendia} were
introduced~\cite{GentlemanAndLang2012} to provide a means of encapsulating the
full scope of the work. Compendia allow readers to see computational details, as
well as re-run computations performed by the author. Gentleman and Lang proposed
that compendia should be used for peer review and distribution of scientific
work~\cite{GentlemanAndLang2012}.

Currently, several tools have been developed for reproducible research
including, but not limited to, Sweave~\cite{Leisch2002},
SASweave~\cite{LenthEtAl2007}, Statweave~\cite{Lenth2009},
Scribble~\cite{FlattEtAl2009}, and Org-mode~\cite{SchulteEtAl2012}. The most
popular of those being Sweave~\cite{SchulteEtAl2012}. The aforementioned tools
maintain a focus on code and certain computational details. Sweave,
specifically, allows for embedding code into a document that is run as the
document is being typeset so that up to date results are always included.
However, Sweave (along with many other tools), still maintains a focus on
producing a single, linear document. Drasil, on the other hand, focuses on
producing multiple, interrelated artifacts from a single knowledge base, providing a breadth of output options and ensuring consistency and reproducibility across all generated outputs.
%\ds{this doesn't belong here}
%It is my hope that Drasil will outperform
%these existing tools due to its flexibility and its ability to create multiple
%artifacts from a knowledge base.

\section{Literate Approaches to Software Development}

There have been several approaches attempting to combine development of program 
code with documentation. Literate Programming and literate software are two 
such approaches that have influenced the direction of this thesis. Each of 
these approaches is outlined in the following sections.

\subsection{Literate Programming}

Literate Programming (LP) is a method for writing software introduced by Knuth 
that focuses on explaining to a human what we want a computer to do rather than 
simply writing a set of instructions for the computer on how to perform the 
task~\cite{Knuth1984}.

Developing literate programs involves breaking algorithms down into
\emph{chunks}~\cite{JohnsonAndJohnson1997} or \emph{sections}~\cite{Knuth1984}
which are small and easily understandable. The chunks are ordered to follow a 
``psychological order''~\cite{PieterseKourieAndBoake2004} if
you will, that promotes understanding. They do not have to be written in the 
same order that a computer would read them. It should also be noted that in a 
literate program, the code and documentation are kept together in one source. 
To extract runnable code, a process known as \emph{tangle} must be performed on 
the source. A similar process known as \emph{weave} is used to extract and 
typeset the documentation.

There are many advantages to LP beyond understandability. As a program is
developed and updated, the documentation surrounding the source code is more 
likely to be updated simultaneously. It has been experimentally found that 
using LP ends up with more consistent documentation and 
code~\cite{ShumAndCook1993}. There are many downsides to having inconsistent 
documentation while developing or maintaining 
code~\cite{Kotula2000,Thimbleby1986}, while the benefits of consistent 
documentation are numerous~\cite{Hyman1990, Kotula2000} including improved 
maintainability and ease of future modifications, reduced risk of introducing 
errors during updates, enhanced onboarding and training for new team members, 
better communication among stakeholders, and increased confidence in overall 
software quality. With the advantages and disadvantages of good documentation 
in mind we see that more effective, maintainable code can be produced if 
properly using LP~\cite{PieterseKourieAndBoake2004}.

Regardless of the benefits of LP, it has not been very popular with 
developers~\cite{ShumAndCook1993}. However, there are
several successful examples of LP's use in SC. Two such literate programs that 
come to mind are VNODE-LP~\cite{Nedialkov2006} and ``Physically Based 
Rendering: From Theory to Implementation''~\cite{PharrAndHumphreys2004} a 
literate program and textbook on the subject matter. Shum and 
Cook~\cite{ShumAndCook1993} discuss the main issues behind LP's lack of 
popularity which can be summed up as dependency on a 
particular output language or text processor, and the lack of flexibility on 
what should be presented or suppressed in the output.

There are several other factors which contribute to LP's lack of popularity and 
slow adoption thus far. While LP allows a developer to write their code and its 
documentation simultaneously, that documentation is comprised of a single 
artifact which may not cover the same material as standard artifacts software 
engineers expect (see Section~\ref{sec:sfs} for more details). LP also does not 
simplify the development process: documentation and code are written as usual, 
and there is the additional effort of re-ordering the chunks. It is also 
challenging to use LP when requirements and design choices change, as those 
changes may affect many different chunks. That is not to say LP does not have 
benefits, the LP development process allows developers to follow a more natural 
flow in development by writing chunks in whichever order they wish, keep the 
documentation and code updated simultaneously (in theory) 
because of their co-location, and automatically incorporate code chunks into 
the documentation to reduce some information duplication.

There have been many attempts to increase LP's popularity by focusing on 
changing the output language or removing the text processor dependency. Several
new tools such as CWeb (for the C language), DOC++ (for C++), noweb 
(programming language independent), and others were developed. Other tools such 
as javadoc (for Java) and Doxygen (for multiple languages) were also influenced 
by LP, but differ in that they are merely document extraction tools. They do 
not contain the chunking features which allow for re-ordering algorithms.

With new tools came new features including, but not limited to, phantom
abstracting~\cite{ShumAndCook1993}, a ``What You See Is What You Get'' (WYSIWYG)
editor~\cite{FritzsonGunnarssonAndJirstrand2002}, and even movement away from 
the ``one source'' idea~\cite{Simonis2003}.

While LP is still not mainstream~\cite{Ramsey1994}, these more robust 
tools helped drive the understanding behind what exactly LP tools must 
do. In certain domains LP is becoming more standardized, for 
example: Agda, Haskell, and R support LP to some extent, even though it is not 
yet common practice. R has good tool support, with the most popular being
Sweave~\cite{Leisch2002}, however it is designed to dynamically create
up-to-date reports or manuals by running embedded code as opposed to being used
as part of the software development process. 

Drasil draws inspiration from LP in its emphasis on integrating documentation 
and code, and in its goal of improving the understandability and 
maintainability of scientific software. Drasil differs from LP in several 
important ways. First, while LP focuses on producing a single, human-readable 
artifact that interleaves code and documentation, Drasil is designed to 
generate multiple, distinct artifacts from a single, unified knowledge base. 
This approach addresses the needs of software engineering processes that 
require a variety of artifacts for
different stakeholders and purposes, going beyond the single-document focus of
traditional LP.

Second, Drasil explicitly models domain knowledge, enabling automated 
consistency across all generated artifacts. In contrast, LP relies on manual 
synchronization between code and documentation. Drasil's knowledge-centric 
approach facilitates reuse, traceability, and automated updates, making it 
better suited for environments where change is frequent and artifact 
consistency is critical. Drasil can be seen as a generalization and extension 
of LP, retaining its core strengths while addressing its limitations.

\subsection{Literate Software}

A combination of LP and Box Structure~\cite{Mills1986} was proposed as a new
method called ``Literate Software Development''
(LSD)~\cite{AlMatiiAndBoujarwah2002}. Box structure can be summarized as the
idea of different views that are abstractions that communicate the same
information in different levels of detail, for different purposes. Box
structures consist of black box, state machine, and clear box structures. The
black box gives an external (user) view of the system and consists of stimuli
and responses; the state machine makes the state data of the system visible (it
defines the data stored between stimuli); and the clear box gives an internal
(designer's) view describing how data are processed, typically referring to
smaller black boxes~\cite{Mills1986}. These three structures can be nested as
many times as necessary to describe a system.

LSD was developed with the intent to overcome the disadvantages of both LP and
box structure. It was intended to overcome LP's inability to specify interfaces
between modules, the inability to decompose boxes and implement the design
created by box structures, as well as the lack of tools to support box
structure~\cite{Deck1996}.

The framework developed for LSD, ``WebBox'', expanded LP and box structures in a
variety of ways. It included new chunk types, the ability to refine chunks, the
ability to specify interfaces and communication between boxes, and the ability
to decompose boxes at any level. However, literate software (and LSD) remains
primarily code-focused with very little support for creating other software
artifacts, in much the same way as LP. Drasil aims to extend the principles of 
LSD by generating a comprehensive suite of software artifacts from a single 
knowledge base. This broader scope addresses the needs of modern software 
engineering practices, which require diverse artifacts for various 
stakeholders, thereby enhancing the utility and applicability of the literate 
approach and making it better suited for environments where artifact diversity 
and traceability are essential.

\section{Generative Programming}

Generative programming is an approach to software development that focuses on 
automating the process of generating code from high-level 
specifications~\cite{Czarnecki2000,Taha2006}. By writing a program 
specification and feeding it to the generator, one does not have to manually 
implement the desired program.

One of the primary benefits of generative programming is that it can help to 
increase productivity and reduce the time and effort required to develop 
software~\cite{Czarnecki2000}. By automating the generation of code, developers 
can focus on high-level design and specification, rather than low-level 
implementation details.

Generative programming has the added benefit of helping to improve the quality 
of software by reducing the risk of errors and 
inconsistencies~\cite{Czarnecki2000,Taha2006}. Since the code is generated 
automatically from high-level specifications, there is less room for human 
error, and the generated code is typically more consistent and predictable.

There are also some potential drawbacks to generative programming. For 
instance, the generated code may not always be optimal or 
efficient~\cite{Czarnecki2000,Taha2006}. As the code is generated 
automatically, it may not take into account all of the nuance or complexity
of the underlying system potentially leading to suboptimal performance or other 
issues.

Generative programming also requires a significant upfront investment in time 
and effort to develop the generators and other tools needed to automate the 
process of code generation~\cite{Czarnecki2000,Taha2006}. This means it is 
often not worth the effort to use generative programming for one-off projects.

There are a large number of generative programming tools available today. Some, 
like template metaprogramming (TMP) tools, are built into a number of 
programming languages (C++, Rust, Scala, Java, Go, Python, etc.) and offer 
varying levels of support for generative programming. Others are widely used 
across a multitude of disparate domains. For example, \textbf{ANTLR} (Another 
Tool for Language Recognition) automatically generates parsers and lexers from 
grammar specifications, enabling rapid development of compilers and 
interpreters~\cite{Parr1995}. \textbf{YACC} (Yet Another Compiler Compiler) and 
its derivatives serve a similar purpose for C-based 
toolchains~\cite{Johnson1975}. In web development, frameworks like 
\textbf{Yeoman} scaffold entire web applications by generating boilerplate 
code, configuration files, and project structure~\cite{Baumgartner2016}. 
\textbf{Swagger Codegen} and the \textbf{OpenAPI Generator} produce client 
libraries, server stubs, and API documentation from OpenAPI specifications, 
streamlining the development of RESTful services~\cite{openapi2025, 
swagger2025}. In embedded systems, tools such as \textbf{Simulink Coder} 
generate C/C++ code from graphical models, facilitating deployment on hardware 
targets~\cite{Chaturvedi2017}. These examples illustrate the breadth of 
generative programming, from language processing to web and embedded systems, 
and demonstrate how code generators can accelerate development and improve 
consistency across software projects.

However, current generators still tend to focus primarily on the code, but the 
same techniques can be used to generate other software artifacts as well. 
Drasil aims to utilize code generation to produce a comprehensive suite of 
software artifacts, from a single knowledge base. This holistic approach 
ensures consistency across all artifacts and addresses the diverse needs of 
modern software engineering practices.