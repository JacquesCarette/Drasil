Here are a few papers (and books) that are relevant:
* One of the papers that got me really thinking along these lines (other than my own work on simplification) was Todd Veldhuizen's [Parsimony Principles for Software Components and Metalanguages](https://arxiv.org/abs/0707.4166). This one should be a must-read. Interestingly, Krzysztof Czarnecki understood the value of this at one point (see the abstract for this talk [Understanding Variability Abstraction](https://link.springer.com/chapter/10.1007/978-3-642-21347-2_1) at ICSR 2011), but didn't seem to pursue it aggressively. Todd had said similar things in [Software Libraries and Their Reuse: Entropy, Kolmogorov Complexity, and Zipf's Law](https://arxiv.org/abs/cs/0508023) where there is actually some empirical data.
* [Compression and Machine Learning: A New Perspective on Feature Space Vectors](https://www.eecs.tufts.edu/~dsculley/papers/compressionAndVectors.pdf)
* Interestingly, I find [Music Analysis and Kolmogorov Complexity](https://pdfs.semanticscholar.org/723d/22af5927ad3612006fc5ded4d86c3abbc7d2.pdf) to be one of the closest papers to what we are attempting.
* The books _Minimum Description Length_ and _Advances in Minimum Description Length_ (both of which I [Jacques] own), are very good.  One can find a pdf of [the first two chapters](https://www.researchgate.net/publication/220489271_A_Tutorial_Introduction_to_the_Minimum_Description_Length_Principle) of the latter book online.

There are various papers that explain the theory, such as
* [Shannon information and Kolmogorov Complexity](https://homepages.cwi.nl/~paulv/papers/info.pdf).  This is a good reference, but is not as helpful as a guide for how to actually come up with good models.
* and the Ph.D. thesis [Minimum Description Length Model Selection](https://www.illc.uva.nl/Research/Publications/Dissertations/DS-2008-07.text.pdf).

These are mathematically interesting, but probably not useful for us:
* [On statistical learning via the lens of compression](https://arxiv.org/abs/1610.03592)