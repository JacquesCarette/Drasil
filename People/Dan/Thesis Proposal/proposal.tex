\documentclass{article}
\usepackage{url}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tikz}
\usepackage{listings}
\usetikzlibrary{positioning}
\newcommand{\colAwidth}{0.2\textwidth}
\newcommand{\colBwidth}{0.7\textwidth}
\begin{document}

\bibliographystyle{plain}
\pagestyle{plain}
\thispagestyle{empty}
\markboth{{\sc thesis proposal}}{{\sc thesis proposal}}
\def\title{\large \sc Literate Software and the Drasil Framework}
\def\author{\sc Dan Szymczak}
\def\studnum{0759661}
\def\email{\sc szymczdm@mcmaster.ca}

\def\degree{Doctor of Philosophy}
\def\deptname{Computing and Software}
\def\supJ{Jacques Carette}
\def\supS{Spencer Smith}

\def\submissiondate{\today}
\def\completiondate{August 2018}

\begin{center}
{\Large \bf 
   McMaster University
\\ Department of \deptname \\}
\vspace{.25in}
{\Large \bf
   Proposal for Thesis Research in Partial Fulfillment
 of the Requirements for the Degree of \degree \\}
\end{center}

\vspace{.5in}

\begin{tabular}{rlc}
   {\large \sc Title:}                       & \multicolumn{2}{l}{\title}
\\ {\sc Submitted by:}
                            & \author  & \\
    {\sc Student Number:}   & \studnum & \\
                            & \email   & \\ 
			    &	       &
\\ { \sc Date of Submission:}          & \multicolumn{2}{l}{\submissiondate}
\\ {\small \sc Expected Date of Completion:} & \multicolumn{2}{l}{\completiondate}
\\ & \\ & \\
\\ {\large \sc Supervisors:}                 & {\large {\supJ}}
\\ & {\large {\supS}}
\end{tabular}

\cleardoublepage
\pagenumbering{roman}
\begin{abstract}

The current state of Scientific Computing software development leaves room for
improvement. There is a need for improved software quality, more consistent use
of software engineering practices, and reduced information duplication. As it
stands, information duplication creates a number of problems, especially when it
comes to software qualities such as traceability and maintainability.
Traceability is hard to achieve, as finding the source of a piece of information,
as opposed to transformations of that source (ex.\ an abstract theory and an
instance model contain transformations of the same knowledge), is difficult
among all of the possibilities. Over time this impacts the maintainability of a
piece of software, as developers may not be able to determine how a change will
affect the rest of the software. Also, with many duplicated information sources,
creating documentation becomes more tedious, reducing a developer's desire to
produce and maintain high-quality documentation. I want to facilitate the
improvement of qualities (maintainability, traceability, reproducibility, etc.)
across all artifacts of a piece of software. I believe a knowledge-based
approach, which borrows ideas from literate programming, and involves a larger
short-term investment from developers is the first step to improving software as
a whole. I am creating a framework (Drasil) to facilitate this approach,
focusing on avoiding knowledge duplication, while improving traceability and
automating as much of the development process as possible. I believe with the
proper tools for automation we can ensure good software engineering practices
are followed (regardless of the developer's background) and improve the overall
quality of software.

\end{abstract}
\pagebreak
\tableofcontents
\pagebreak
\pagenumbering{arabic}

\section{Introduction}

%I want everyone to make better software. I believe this can be achieved by
%improving software qualities such as maintainability, traceability,
%verifiability, reproducibility, and reusability. As that is too broad of a
%target, I will be focusing on well-understood domains; specifically Scientific
%Computing (SC). SC has a very rich, well-understood background, but tends to
%suffer from a lack of high-quality software and tools. In many cases, SC
%developers are scientists first and lack a strong software engineering
%background.

Scientists and engineers rely heavily on models that share common
mathematical/scientific knowledge. These are shared, in textbook form, across
many projects. However, when it comes to implementing software, this information
is then duplicated for each project, as well as being duplicated within the
project (in requirements, design documents, code and tests, for example). It
should be possible to capture this knowledge once, and then re-use it, first
within the same project but in different views (documents/contexts), then across
many projects. I want to create a system that deals with this issue of
information duplication, especially for the knowledge contained in scientific
models.

As it stands, information duplication can have a direct impact on qualities such
as maintainability, traceability, verifiability, and reproducibility. Consider a
project containing (at least) these four software artifacts: requirements
document, design document, source code, and a testing document. As the project
evolves, some of the knowledge contained within it will inevitably change due to
changing requirements, new design decisions, technical or resource limitations,
etc. Updating the software artifacts to reflect these changes is tedious; a
developer must trace their way through each of the artifacts to ensure new
information has been added (new requirements and/or design decisions, code
implementation, and test cases), the change does not cause conflicts,
%with existing requirements/design/assumptions/test cases 
and any existing test cases or assumptions are still applicable (or are
appropriately removed/updated).

Now consider a software project which follows a rational design process as seen
in Figure~\ref{fig:proc}. It consists of even more software artifacts, thus each
change to the knowledge becomes even more tedious to propagate. It is not
uncommon for developers to lack the resources or motivation to update each
artifact to accommodate every minor change.

\begin{figure}
\label{fig:proc}
\includegraphics[width=.95\textwidth]{OverviewOfProcess.pdf}
\caption{Example Rational Design Process and its Software Artifacts}
\end{figure}

As artifacts in a software project begin to fall out of sync with each other,
the qualities of the project as a whole suffer. Consider traceability: if the
artifacts in a software project are inconsistent with each other, how can one
accurately analyze a change or validate a requirement? On a similar note, how
easy is it to maintain a system if one cannot track where a problem came from?
Also, certification becomes impossible until all of the artifacts have been
updated, as there is no way to verify/validate the software's correctness. I
want to create a system that allows developers to maintain and update software
artifacts at very little cost.

Up-to-date software artifacts are also incredibly important for scientists as a
whole. Good science relies on reproducible results; being able to reproduce the
exact piece of software, as well as any inputs used in running a simulation is
vital. Due to library version changes, minor hacks, and undocumented
modifications, it is often difficult for other scientists to reproduce the exact
results from a given simulation\cite{IonescuAndJansson2013}.
%CITE IONESCU
Would it not lead to higher confidence if we
could reproduce and analyze the results of the original experiment? I want to
help scientists ensure their work is easily reproducible by giving them a means
of sharing how their software was built, as well as any necessary input or
configuration data.

%How to bridge this gap? TODO

I think the first step to improving software as a whole is to improve each of
the software artifacts. To that end, I would like to expand the ideas of
literate programming (which I will discuss more in-depth in
Section~\ref{sec:lp}). Specifically, I want to keep the idea of \textit{chunks}
and expand its application to not only code snippets, but the fundamental
knowledge underlying a piece of software. I believe capturing this knowledge
will be paramount to creating a new, so-called ``knowledge-based'' approach to
software development.

As improving all software is too broad of a target, I will be focusing on
well-understood domains. I believe this is a necessary restriction because a
knowledge-based approach will require a thoroughly well-understood background to
facilitate the requisite knowledge capture. To that end, I will focus
specifically on the domain of Scientific Computing (SC). SC has a very rich,
well-understood background, which has already been documented in incredible
detail. Also, in many cases, SC developers are scientists first and lack a
strong software engineering background. It is my hope that my approach will
be well-suited to SC developers as it will allow them to focus more of their
energy on the science than the software development.

I do not believe I am creating any individual new ideas. Instead, I want to
combine ideas that have existed for some time and use them in a novel way. That
is where the idea of a knowledge-based approach to literate software comes from.
Not only do I want to create better software using a process similar to literate
programming, I want to ensure that process will cover all necessary software
artifacts.

To that end, I believe a tool will be necessary to facilitate my approach. 
A framework known as \emph{Drasil} is currently in development and will be explained in 
Section~\ref{sec:Drasil}. 

%Development of Drasil is following a practical,
%example-driven approach in an attempt to not get bogged down in design for
%too long. With periodic overhauls and continuous iteration, I believe I
%will avoid the majority of pitfalls related to underdevelopment of a 
%design.

%Drasil employs techniques from literate programming alongside code generation to
%automate as much as possible in the software development process.

\section{State of the Art}

To pursue this line of research, first one needs to understand the history of
research in several closely-related areas. Since I will be focusing on the
improvement of SC software, the obvious starting point would be to look at the
current state of SC software development and its challenges. Also, as I am
intending to expand the ideas of Literate Programming (LP), it is necessary to
delve into sufficient depth on LP and why it has not been widely adapted as yet,
as well as any attempts to expand or build upon it. Finally, with the idea of
long-term maintainability, full traceability, and reproducibility in mind, I
would be remiss if I did not look into the field of reproducible research.

\subsection{Current State of SC Software Development}

In many instances of SC software development, the developers are the scientists.
These developers tend to put the most emphasis on their science instead of good
software development practices~\cite{Kelly2007}. Rigid, process-heavy approaches
are typically considered unfavourable to these developers~\cite{CarverEtAl2007}.
We see the developers choose to use an agile philosophy~\cite{AckroydEtAl2008,
CarverEtAl2007, EasterbrookAndJohns2009, Segal2005}, an
amethododical~\cite{Kelly2013}, or a knowledge acquisition driven
process~\cite{Kelly2015} instead.

There are several clear problems with the current state of SC software
development. The first, fairly obvious problem is that knowledge reuse is not
being utilized to the fullest extent possible. As an example, a
survey~\cite{Owen1998} showed that of 81 different mesh generator packages, 52
generated triangular meshes. Now that in itself may not show a lack of knowledge
reuse, however, looking deeper we see that 37 of those packages used the same
Delaunay triangulation algorithm for generating those meshes. There is no reason
that the exact same algorithm should be implemented 37 separate times when it
could simply be reused.

Another problem in SC software development is the lack of understanding of
software testing. More than half the scientists developing SC software lack a
good understanding of software testing~\cite{Merali2010}. It is in such a bad
state that quality assurance has ``a bad name among creative scientists and
engineers''~\cite[p.~352]{Roache1998}, not to mention the very limited use of
automated testing~\cite{PatrickEtAl2015}.

It should be obvious that some of these issues could be solved through the use
of certain tools. However, it should be noted that tool use by SC software
developers is also very limited, especially the use of version control
software~\cite{Wilson2006}.

Not everything about SC software development today is a negative. For example
advanced techniques like code generation have been quite successful in SC. Some
generation examples that come to mind are FEniCS~\cite{LoggEtAl2012},
FFT~\cite{KiselyovEtAl2004}, Gaussian Elimination~\cite{Carette2006}, and
Spiral\cite{PuschelEtAl2005}. The focus of generation techniques, thus far, have
been solely on one software artifact: the source code. Focusing solely on code
is a disadvantage to SC software developers as the value of documentation, as
well as a structured (or rational) process, have been repeatedly
illustrated~\cite{SmithAndKoothoor2016, SmithEtAl2015-SS-TR, SmithEtAl2015SQJ,
SmithEtAl2013}.

\subsection{Literate Programming}
\label{sec:lp}

The LP method introduced by Knuth changes the focus from writing programs
that simply instruct the computer on how to perform a task to explaining
(\emph{to humans}) what we want the computer to do~\cite{Knuth1984}.

Developing literate programs involves breaking algorithms down into
\emph{chunks}~\cite{JohnsonAndJohnson1997} or \emph{sections}~\cite{Knuth1984}
which are small and easily understandable. The chunks are ordered to promote
understanding, a ``psychological order''~\cite{PieterseKourieAndBoake2004} if
you will. They do not have to be written in the same order that a computer would
read them. It should also be noted that in a literate program, the code and
documentation are kept together in one source. To extract working source code, a
process known as \emph{tangle} must be run on the source. A similar process
known as \emph{weave} is used to extract and typeset the documentation from the
source.

There are many advantages to LP beyond understandability. As a program is
developed and updated, the documentation surrounding the source code tends to be
updated simultaneously. It has been experimentally found that using LP ends up
with more consistent documentation and code~\cite{ShumAndCook1993}. Having
consistent documentation has its own advantages while developing or maintaining
software~\cite{Hyman1990, Kotula2000}. Similarly, there are many downsides to
inconsistent documentation~\cite{Kotula2000,Thimbleby1986}. Keeping both of
those in mind we can see that more effective, maintainable code can be produced
when (properly) using LP~\cite{PieterseKourieAndBoake2004}.

Even with all of the benefits of LP, it has not been very
popular~\cite{ShumAndCook1993}. Still, there are
several successful examples of LP's use in SC. Two such examples are
VNODE-LP~\cite{Nedialkov2006} and ``Physically Based Rendering: From Theory to
Implementation''~\cite{PharrAndHumphreys2004}. The latter being a literate
program as well as a textbook. Shum and Cook discuss the topic of LP's lack of
popularity and present the idea that it comes from a couple of main issues:
dependency on a particular output language or text processor, and the lack of
flexibility on what should be presented or suppressed in the output.

%--------------------------------------------------------------------------------
%More edits?

I believe there are several other factors which contributed to LP's lack of
popularity and slow adoption thus far. LP allows a developer to write their code
and its documentation simultaneously. However, that documentation is comprised
of a single document which does not necessarily cover the same material as the
standard artifacts software engineers expect. LP also does not really simplify
the development process: documentation and code must be written as usual, and
there is the added effort of re-ordering the chunks. The only major benefits to
the development process are that the chunks can be written in any order
(allowing a more natural flow in development), the documentation and code are
(in theory) updated simultaneously, and chunks can be automatically incorporated
into the documentation (reducing some information duplication).

%--------------------------------------------------------------------------------

Many attempts to address the issues with LP's popularity have focused on
changing or removing the output language or text processor dependency. Several
new tools were developed such as: CWeb (for the C language), DOC++ (for C++),
noweb (programming language independent), and more. Tools such as javadoc (for
Java) and Doxygen (for multiple languages) were also influenced by LP, but
differ in that they are merely document extraction tools. They do not contain
the chunking features which allow for re-ordering algorithms.

The development of new tools led to the introduction of many new features
including, but not limited to, a ``What You See Is What You Get'' (WYSIWYG)
editor~\cite{FritzsonGunnarssonAndJirstrand2002}, phantom
abstracting~\cite{ShumAndCook1993}, and even movement away from the ``one
source'' idea~\cite{Simonis2003}.

While these tools did not bring LP into the mainstream~\cite{Ramsey1994}, they
did help drive the understanding behind what exactly LP tools must do. We can
now see LP becoming more standardized in certain domains (for example: Agda,
Haskell, and R support LP to some extent), even though it is not yet common
practice. R has good tool support, with the most popular being
Sweave~\cite{Leisch2002}, however it is designed to dynamically create
up-to-date reports or manuals by running embedded code as opposed to being used
as part of the software development process. I intend to use the knowledge
gained from the history of LP to develop the Drasil framework into a
full-featured and useful tool.

\subsection{Literate Software}

A combination of LP and Box Structure~\cite{Mills1986} was proposed as a new
method called ``Literate Software Development''
(LSD)~\cite{AlMatiiAndBoujarwah2002}. Box structure can be summarized as the
idea of different views which are abstractions that communicate the same
information in different levels of detail, for different purposes. Box
structures consist of black box, state machine, and clear box structures. The
black box gives an external (user) view of the system and consists of stimuli
and responses; the state machine makes the state data of the system visible (it
defines the data stored between stimuli); and the clear box gives an internal
(designer's) view describing how data are processed, typically referring to
smaller black boxes~\cite{Mills1986}. These three structures can be nested as
many times as necessary to describe a system.

LSD was developed with the intent to overcome the disadvantages of both LP and
box structure. It was intended to overcome LP's inability to specify interfaces
between modules, the inability to decompose boxes and implement the design
created by box structures, as well as the lack of tools to support box
structure~\cite{Deck1996}.

%The main idea behind LSD was to overcome the disadvantages of LP and box
%structure. These disadvantages included: the inability of LP to specify
%interfaces between modules; the lack of ability to decompose boxes; a lack of
%tools to support box structure~\cite{Deck1996}; a lack of ability to implement
%the high-level analysis and design created using box structures.

The framework developed for LSD, ``WebBox'', expanded LP and box structures in a
variety of ways. It included new chunk types, the ability to refine chunks, the
ability to specify interfaces and communication between boxes, and the ability
to decompose boxes at any level. However, literate software (and LSD) remains
primarily code-focused with very little support for creating other software
artifacts, in much the same way as LP. This is one area where I believe Drasil
will outperform WebBox.

\subsection{Reproducible Research}

Being able to reproduce results, is fundamental to the idea of good science.
When it comes to software projects, there are often many undocumented
assumptions or modifications (including hacks) involved in the finished product.
This can make replication impossible without the help of the original author,
and in some cases reveal errors in the original author's
work~\cite{IonescuAndJansson2013}.

Reproducible research has been used to mean embedding executable code in
research papers to allow readers to reproduce the results
described~\cite{SchulteEtAl2012}.

Combining research reports with relevant code, data, etc.\ is not necessarily
easy, especially when dealing with the publication versions of an author's work.
As such, the idea of \emph{compendia} were
introduced~\cite{GentlemanAndLang2012} to provide a means of encapsulating the
full scope of the work. Compendia allow readers to see computational details, as
well as re-run computations performed by the author. Gentleman and Lang proposed
that compendia should be used for peer review and distribution of scientific
work~\cite{GentlemanAndLang2012}.

Currently, several tools have been developed for reproducible research
including, but not limited to, Sweave~\cite{Leisch2002},
SASweave~\cite{LenthEtAl2007}, Statweave~\cite{Lenth2009},
Scribble~\cite{FlattEtAl2009}, and Org-mode~\cite{SchulteEtAl2012}. The most
popular of those being Sweave~\cite{SchulteEtAl2012}. The aforementioned tools
maintain a focus on code and certain computational details. Sweave,
specifically, allows for embedding code into a document which is run as the
document is being typeset so that up to date results are always included.
However, Sweave (along with many other tools), still maintains a focus on
producing a single, linear document. It is my hope that Drasil will outperform
these existing tools due to its flexibility and its ability to create multiple
artifacts from a knowledge base.

\section{Research Objectives and Approach}
\subsection{Objectives}

Looking at the current state of SC software development, there are many areas
for improvement particularly the need for applying better software engineering
practices and improving knowledge reuse. 
%The simplest approach for improving
%development would be to get better developers, which is not necessarily feasible
%for SC software. Software engineering education is not typically part of an SC
%developer's background, and having software engineers develop SC software is
%problematic because the software engineers typically lack the necessary domain
%knowledge. 
Domain experts require tools that simplify the software engineering allowing
them to do things the ``right'' way. My first objective, therefore, is to
simplify the software development process by allowing scientists to focus on the
science. This objective, however, will not be achievable during the time frame I
have allotted for my thesis, so I believe a more focused and attainable
objective would be to create tool(s) which will enable knowledge-driven SC
software development.

My second objective, is the production of better overall
software artifacts (requirements and design documents, source code, etc.). It is
a worrying trend that software artifacts tend to fall out of sync with each
other over time, so ensuring they remain consistent is a means to improve.

Furthermore, developers should not need to spend large swaths of time updating
and maintaining their software. I want to automate as much of the software
development process as possible. Automation will also allow developers to avoid
classical development mistakes such as duplication, previously mentioned in
the survey by Owen~\cite{Owen1998}. It will also ensure reproducibility as, if
done right, anyone will be able to automatically create and run the same
software easily.

\subsection{Approach}

To meet my objectives I am using a combination of existing ideas, which I have
dubbed a \textit{knowledge-based} approach to software development. First off, I
want to expand and use ideas from LP. Namely, I want to keep the brilliant idea
of chunks, however, it should be expanded to represent encapsulated knowledge,
not just code. If the knowledge behind an underlying concept can be properly
encapsulated, then it can easily be reused and duplication can be avoided.

Knowledge encapsulation should be done at the specification level instead of the
code level as was previously done in LP. It will include assumptions,
derivations, equations, etc.~and from there it will be a fairly simple and
straightforward process to get a code representation. Specifically, the
high-level knowledge can be used for the automatic generation of code. I believe
this will simplify the development of software artifacts by automatically
allowing captured knowledge to be transformed and reused as necessary.

The standard LP style helps to increase consistency in documentation, but it
does not go far enough. It is easy for a developer to update the code and/or
documentation, however, there is no way to ensure that when the code is changed
the documentation is also updated accordingly. I want to ensure that all
artifacts are updated any time there is a change (no matter how trivial) with
trivial effort. 

This automatic updating is achieved through generation. The idea is to have a
standard generator which uses \textit{recipes} for the creation of artifacts.
Each recipe essentially defines a different view of our knowledge-base (chunks).
Since each recipe represents a different software artifact and pulls appropriate
knowledge from reusable chunks (see Figure~\ref{fig:tree}), the artifacts will
remain consistent and fully traceable; finding the source of a piece of
information will be trivial. All artifacts will automatically be updated any
time the generator is run, thus ensuring consistency is maintained. I believe
this is one of the key features that will set my approach apart from those that
have come before and will make it all the more useful for (SC) software
developers.

\begin{figure}
\includegraphics[width=\textwidth]{tree.png}
\caption{Recipes pull relevant information from chunks (the roots of the tree) and combine them to create software artifacts (the leaves of the tree).}
\label{fig:tree}
\end{figure}

I also believe my approach will be useful for scientists because anyone who is
given access to the recipes and chunks used in creating a piece of software will
be able to reproduce that software exactly. 

Currently, I am taking a practical, example-driven approach to create a
framework, Drasil, to exemplify this knowledge-based approach to software
development. Thus far, the implementation of Drasil has been guided by some
small and fairly specific case studies. However, I plan to continue expanding
the framework through the use of larger, more varied case studies. More
information regarding Drasil can be found in the next section.

\section{Current Work and Preliminary Results}
\label{sec:Drasil}

\subsection{The Drasil Framework}

Drasil is currently being implemented as a combination of embedded Domain
Specific Languages (eDSLs) in Haskell. Currently, six eDSLs have been implemented
as follows:

\begin{enumerate}
\item Expression language -- A simple expression language that allows one to
capture knowledge relating to equations and mathematical operations. It includes
operations such as addition, multiplication, derivation, and exponentiation
among others.

\item Expression layout language -- A micro-scoped language for describing how
expressions should appear. Expressions may need to use a variety of inline
layout modifiers (subscripts, superscripts, etc.) to be properly displayed.

\item Document layout language -- A macro-scoped language for describing how
large-scale layout objects (tables, sections, figures, etc.) should appear.

\item C Representation Language -- A DSL for representing parts of the C
programming language inside the Drasil framework. This allows the generator to
produce working C code.

\item \LaTeX{} Representation Language -- A DSL for representing \LaTeX{} code
inside of Drasil. As with the C representation, it is used by the generator to
produce working \LaTeX{} code.

\item HTML Representation Language -- A DSL for representing HTML within Drasil.
Similar to the other representation languages as it is used by the
generator to produce working HTML.
\end{enumerate}

With these eDSLs it is easy to encapsulate knowledge, create recipes, and
generate required artifacts.

When it comes to chunks, they come in several varieties. You can see a hierarchy
of chunk types in Figure~\ref{fig:chunks}.

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=6mm]
  \tikzstyle{every node}=[draw,shape=rectangle, rounded corners,
    text width=6em, text centered];
  \node (ch)                     	{Chunk (\emph{name})};
  \node (co) [below = of ch]       {Concept (\emph{description})};
  \node (qu) [below = of co]  		{Quantity (\emph{symbol})};
  \node (u ) [right = of qu] 		{Unit (\emph{unit})};
  \node (uc) [below = of qu] 		{Unital};
  \node (eq) [below = of uc]	{EqChunk (\emph{equation})};
  \node (rc) [left = of eq]	{RelationChunk (\emph{relation})};

  \draw [->, line width=2pt] (ch) -- (co);
  \draw [->, line width=2pt] (co.west) -- (-2.96,-1.6) -- (rc); 
		%No idea how to do this
  \draw [->, line width=2pt] (co) -- (qu);
  \draw [->, line width=2pt] (co.east) -- (2.96,-1.6) -- (u );
  \draw [->, line width=2pt] (qu) -- (uc);
  \draw [->, line width=2pt] (u .south) -- (2.96,-4.54) -- (uc);
  \draw [->, line width=2pt] (uc) -- (eq);
\end{tikzpicture}
\end{center}
\caption{Our current chunk design (parentheses indicate newly added knowledge)}
\label{fig:chunks}
\end{figure}

Each new chunk in the hierarchy adds to the knowledge encapsulated within it.
The most basic \textit{Chunk} represents a named piece of information. A
\textit{Concept} adds a description to the named information, and so on.

\textit{Unital} chunks are somewhat special as they do not add any new
information in and of themselves, they act as a combination of a
\textit{Quantity} (concept with a symbolic representation) and a \textit{Unit}.
They are quantities with units.

\textit{Equation} chunks continue to expand on \textit{Unital} chunks by
allowing us to capture equations for calculating quantities. In a similar vein
are \textit{Relation} chunks, which allow us to relate two pieces of knowledge to
each other.

Now that knowledge has been encapsulated it is time to do something with it.
This is where the recipes, mentioned earlier, come into play. Writing a recipe
requires using three of the previously mentioned eDSLs -- the expression,
expression layout, and document layout languages. I will go into more detail on
writing and using recipes in the next section.

Finally, the last piece of the implementation is the generator. The remaining
three representation languages are specific to the generator. The generator
interprets the recipes and creates intermediary representations of artifacts
using the representation languages, then pretty-prints the results.

\subsection{Using Drasil}

For this section I will touch on one of the case studies that I have used to
motivate the development of Drasil. It is a simplified version of a Software
Requirements Specification (SRS) for a fuel pin in a nuclear
reactor~\cite{SmithAndKoothoor2016}. While other case studies are being used to
help motivate Drasil's development, they will not be shown explicitly at
this time, as their results can be seen using the fuel pin example.

Let's start by looking at a single term from the fuel pin SRS: $h_c$. This term
represents the convective heat transfer coefficient between the clad and
coolant. We can see the data definition for this term in Figure~\ref{fig:h_c}.

\begin{figure}
~\newline \noindent \begin{minipage}{\textwidth}
\begin{tabular}{p{\colAwidth} p{\colBwidth}}
\toprule \textbf{Number} & \textbf{DD2 \label{hc}}
\\ \midrule 
Label & 
$h_{c}$
\\ \midrule
SI Units & $\mathrm{\frac{kW}{m^{2o}C}}$\\ \midrule
Equation & $h_{c}$ =$
\frac{2k_{c}h_{b}}{2k_{c}+\tau_{c}h_{b}}$\\ \midrule
Description & $h_{c}$ is the convective heat transfer coefficient between clad
and coolant
\newline
$k_{c}$ is the
clad conductivity \newline
$h_{b}$ is the
initial coolant film conductance \newline
$\tau_{c}$ is the
clad thickness 
\newline
%NOTE: Equation taken from the code\\ \midrule  Sources & source code \\ \bottomrule 
\end{tabular} \end{minipage}\\ 
\caption{Data definition for $h_c$ from the fuel pin SRS}
\label{fig:h_c}
\end{figure}

From the data definition, we can glean interesting knowledge. First off there is
the name of the concept, its description, the symbol representing it, its SI
units, and its defining equation. All of this information can be encapsulated
into an EqChunk with ease as shown in Figure~\ref{fig:h_c_chunk}. Note that the
units for $h_c$ are defined in a piece of common knowledge known as
\verb|heat_transfer|.

\begin{figure}
\begin{lstlisting}[frame=single, showstringspaces=false, basicstyle=\small]
h_c_eq :: Expr
h_c_eq = 2*(C k_c)*(C h_b) /
  (2*(C k_c) + (C tau_c)*(C h_b))

h_c :: EqChunk
h_c = fromEqn "h_c" 
 "convective heat transfer coefficient
    between clad and coolant"
 (sub h c) heat_transfer h_c_eq
\end{lstlisting}
\caption{The $h_c$ chunk in Drasil}
\label{fig:h_c_chunk}
\end{figure}

The SI Units are a great example of common knowledge. The SI Unit library
contains all seven base SI units and several different derived units (for
example degrees Celsius, which are derived from Kelvin). The seven fundamental
base SI units implemented in Drasil can be seen in Figure~\ref{fig:SI}.

\begin{figure}
\begin{lstlisting}[frame=single, showstringspaces=false, basicstyle=\small, linewidth=13cm]
metre, second, kelvin, mole, kilogram, ampere, candela :: FundUnit
metre    = fund "Metre"    "length (metre)"               "m"
second   = fund "Second"   "time (second)"                "s"
kelvin   = fund "Kelvin"   "temperature (kelvin)"         "K"
mole     = fund "Mole"     "amount of substance (mole)"   "mol"
kilogram = fund "Kilogram" "mass (kilogram)"              "kg"
ampere   = fund "Ampere"   "electric current (ampere)"    "A"
candela  = fund "Candela"  "luminous intensity (candela)" "cd"
\end{lstlisting}
\caption{The seven fundamental SI Units in Drasil}
\label{fig:SI}
\end{figure}

From the captured knowledge and common knowledge, it is now possible to start
putting together a document using recipes. A small portion of the recipe for the
simplified fuel pin SRS can be seen in Figure~\ref{fig:recipe}. Currently the
recipes are fairly clunky and rely heavily on Haskell, but in the future this
will be improved. However, even with the current recipes we can see that the
output from the generator (Figure~\ref{fig:out}) is exactly what we want. Note:
the output I am showing is from the HTML version being generated, the TeX
version looks (almost) identical to the original SRS.

\begin{figure}[tb]
\begin{lstlisting}[frame=single, 
  showstringspaces=false, basicstyle=\scriptsize]
srsBody = srs [h_g, h_c] "Spencer Smith" [s1,s2,s3]

s1 = Section (S "Table of Units") [intro, table]

table = Table 
 [S "Symbol", S "Description"] (mkTable
   [(\x -> Sy (x ^. unit)),
    (\x -> S (x ^. descr)) ] si_units)

intro = Paragraph (S "Throughout this ...")
\end{lstlisting}
\caption{A portion of our simplified SRS recipe}
\label{fig:recipe}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.6]{HTML_out.png}
\caption{Section 1 of the generated SRS (HTML version)}
\label{fig:out}
\end{figure}

Already several advantages of using Drasil and a knowledge based approach become
apparent First off, there are no inconsistencies in the knowledge both within
and across artifacts. Manually copying knowledge is no longer necessary, and we
can easily trace where a piece of knowledge came from. Should anything need
updating, the updates will automatically propagate throughout the artifacts when
the generator is run. We also have completely reusable knowledge (for example:
the SI Units) and the use of recipes supports design for change. As long as
knowledge is captured properly, it is as simple as changing a configuration file
to change the software.

One advantage that may seem like a disadvantage (but is not) is that of
pervasive bugs. Thanks to the full traceability and reuse of knowledge, a single
bug will be reproduced anywhere that piece of knowledge is necessary. This
improves the odds of finding it and makes it simple to fix. A single change
should update all of the artifacts and remove the bug.

There are a few disadvantages to using the knowledge based approach with Drasil.
The most obvious is the inability to include local hacks. Any human-modified
generated file will lose its modifications the next time the generator is run.
Creating common-knowledge libraries is also fairly difficult as it requires the
involvement of domain experts.

\section{Work Plan and Next Steps}

A full schedule of my work plan can be found in Table~\ref{table:sum_schedule}.
There are still many features I would like to add to Drasil as well as
improvements to the overall implementation. Currently anticipated additions and
changes (in no particular order) are as follows:

\begin{itemize}
\item Encapsulate more types of information in chunks. 
	Some of the next additions should be physical constraints and reasonable values.
\item Use constraints to generate test cases.
\item Implement much larger examples.
\item Generate code in more languages. Specifically MATLAB is the next planned output 
		language implementation.
\item Generate more artifact types. As it stands, the current recipes only create 
		requirements documents or code. New recipes should be included to cover design 
		documents, test cases, build instructions, user manuals, and more.
\item Generate different document views. This is partially implemented in the requirements 
		document recipe by allowing simplified or verbose data descriptions. The ability to 
		simplify parts of the document that are unnecessary for a target audience should be 
		expanded.
\item Create an external syntax for Drasil.
\end{itemize}

Over the summer (2016), three undergraduate students will be working on
translating existing implementations of large examples into Drasil
implementations. It is my hope that this experiment will provide insight on
any lacking features of Drasil, as well as how well the new approach works. A
second PhD student will also be joining the project over the summer. His first
task will involve implementing a large example and helping to expand Drasil.

\begin{table}
\caption{A detailed work schedule for the next twenty-eight months}
\begin{tabular}{|l|p{0.85\textwidth}|}
\hline
      Summer 2016 & Summer student experiment.
            Implement multiple (3-4) large examples using Drasil, updating the
              framework as new needs for features arise.
               
	Write up results of experiment as a paper and submit to SEHPCCSE'16.
				\\ \hline
      Fall 2016 & Overhaul the Drasil back-end to solidify necessary features
                      and re-design parts of the implementation.
                      
                      Finish PhD course requirements. 
                      
                      Meet with Ernie from OPG. 
\\ \hline
      Winter 2016 & Implement external syntax for Drasil. 

		Write a paper for ICSR (International Conference on Software Reuse).
      
      Write a journal paper for Automated Software Engineering.
\\ \hline
      Spring 2017 & Re-evaluate current Drasil implementation for usability. 
        Work on making it as user-friendly as possible. 

		Submit a paper to Onward!
		
		Submit a paper to ASE (International Conference on Automated Software Engineering)
        
      Meet with committee. 
\\ \hline
      Summer 2017 & Write a paper.
      
      Attempt to get summer students for second round of 
        experimentation.
        
        Write a journal paper detailing results.
\\ \hline
      Fall 2017 & Meet with Ernie from OPG. 
      
								 Update Drasil and perform a final evaluation. 			 
\\ \hline
      Winter 2017 & Begin writing up full PhD Thesis.
\\ \hline
      Spring 2018 & Complete first draft of thesis and send to supervisors for 
        comments. Complete first round of edits.
\\ \hline
      Summer 2018 & Complete final thesis draft before defense. Make any
      necessary revisions to the thesis and submit it.
\\ \hline
\end{tabular}
\label{table:sum_schedule}
\end{table}


\newpage
\bibliography{drasil}
\newpage
%\appendix
%\section{Appendix}
%\label{app}
%\begin{table}[htp!]
%\caption{A detailed work schedule for the next twenty-eight months}
%\begin{tabular}{|l|p{0.85\textwidth}|}
%\hline
%      Summer 2016 & Summer student experiment.
%            Implement multiple (3-4) large examples using Drasil, updating the
%              framework as new needs for features arise.\\ \hline
%      June/July 2016 & Implement more artifact types in Drasil
%\\ \hline
%      August 2016 & Submit results of summer experimentation to SEHPCCSE'16
%\\ \hline
%      September 2016 & Overhaul the Drasil back-end to solidify necessary features
%                      and re-design parts of the implementation.
%\\ \hline
%      September 2016 & Begin the last course for my PhD requirements.
%\\ \hline
%      October 2016 & Meet with Ernie from OPG.
%\\ \hline
%      October 2016 & Begin implementation of more knowledge capture.
%\\ \hline
%      November 13, 2016 & (Hopefully) Present work at SEHPCCSE'16.
%\\ \hline
%      December 2016 & Finish coursework.
%\\ \hline
%      December 2016 & Write a paper.
%\\ \hline
%      January 2017 & Begin implementation of auto-generated test cases.
%\\ \hline
%      January 2017 & Begin work on external syntax.
%\\ \hline
%      February 2017 & Write a paper to submit to a journal.
%\\ \hline
%      March 2017 & Re-evaluate current Drasil implementation for usability. 
%        Work on making it as user-friendly as possible
%\\ \hline
%      April 2017 & Find and collect more large-scale examples to be implemented 
%        and begin implementation of one.
%\\ \hline
%      April 2017 & Meet with my committee.
%\\ \hline
%      May 2017 & Write a paper.
%\\ \hline
%      Summer 2017 & Attempt to get summer students for second round of 
%        experimentation. Complete implementation of multiple large-scale 
%        examples.
%\\ \hline
%      August 2017 & Write a paper to submit to a journal detailing the results.
%\\ \hline
%      September 2017 & Meet with Ernie from OPG.
%\\ \hline
%      Sept - Nov 2017 & Update Drasil and perform a final evaluation.
%\\ \hline
%      Dec 2017 & Begin writing up full PhD Thesis.
%\\ \hline
%      February 2018 & Complete first draft of thesis and send to supervisors for 
%        comments.
%\\ \hline
%      March 2018 & Begin editing thesis.
%\\ \hline
%      May 2018 & Complete final thesis draft before defense.
%\\ \hline
%      June 21, 2018 & Defend thesis.
%\\ \hline
%      July-August 2018 & Make any necessary revisions to the thesis, 
%        if major: defend again;
%        if minor: submit the finished copy.
%\\ \hline
%\end{tabular}
%\label{table:schedule}
%\end{table}
\end{document}




