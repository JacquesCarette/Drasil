\chapter{A look under the hood: \mbox{Our process}}
\label{c:process}

%\ds{Make sure we talk about continuous integration / git processes / etc.
%Actually might belong in the next chapter - iteration and refinement}

The focus of this chapter is the process by which we developed the Drasil 
framework itself. Rather than describing the general software development 
process that Drasil is intended to support or automate, we turn our attention 
inward to examine the steps, decisions, and rationale that guided the creation 
of the framework. Our goal is to provide transparency into the reasoning that 
shaped Drasil, highlighting the challenges encountered and the strategies 
employed to address them. By doing so, we aim to give the reader a clear 
understanding of how the framework's design emerged from a careful analysis of 
redundancy and knowledge organization across software artifacts.

As one of the central motivations for developing Drasil was the observation 
that \sfs{} contain significant, unnecessary redundancy, our first step in 
removing that redundancy is identifying exactly what it is and where it exists. 
To that end we need to understand what each of our software artifacts is 
attempting to communicate, who their audience is, and what information can be 
considered boilerplate versus system-specific. Luckily, we have an excellent 
starting point thanks to the work of many smart people: artifact templates.

% Lots of work \ds{cite some people who did this} has been done to specify 
% exactly what should be documented in a given artifact in an effort for 
% standardization. Ironically, this has led to many different `standardized' 
% templates. Through the examination of a number of different artifact templates, 
% we have concluded they convey roughly the same overall information for a given 
% artifact. Most differences are stylistic or related to content 
% organization and naming conventions.

% Once we understand our artifacts, we take a practical, example-driven approach
% to identifying redundancy through the use of existing software system case
% studies. For each of these case studies, we start by examining the source code
% and existing software artifacts to understand exactly what problem they are
% trying to solve. From there, we attempt to distill the system-specific knowledge
% and generalize the boilerplate.

\section{A minimal motivating example: HGHC}
\label{sec:hghc-mini}

Before delving into the full complexity of our case studies and artifact 
templates, it is helpful to illustrate the core problem of knowledge redundancy 
with an incredibly minimal, toy example. For this, we use the ``HGHC'' system, 
a simple heat transfer case study included in the repository.

The HGHC example's SRS specifies what problem the software is trying to solve/ 
In this case, it should model heat transfer coefficients in nuclear fuel rods, 
specifically $h_g$ (gap conductance) and $h_c$ (effective heat transfer 
coefficient between clad and coolant). Even in this minimal system, the same 
core knowledge appears in multiple places within the SRS.

To illustrate, consider the following: The SRS defines variables such as $h_g$, 
$h_c$, $\tau_c$ (clad thickness), $h_p$ (initial gap film conductance), $h_b$ 
(initial coolant film conductance), and $k_c$ (clad conductivity). The defining 
equations for $h_g$ and $h_c$ are also given as:
    \[
      h_g = \frac{2k_{c}h_{p}}{2k_{c}+\tau_c h_{p}}
    \]
    \[
      h_c = \frac{2k_{c}h_{b}}{2k_{c}+\tau_c h_{b}}
    \]

We can already see repeated knowledge ($k_c$ and $\tau_c$) in these two 
equations. But the redundancy goes even deeper in the full SRS (which can be 
found in Appendix~\ref{appendix:hghc}). There we can see repeated descriptions 
and units for $h_g$ and $h_c$ in the table of symbols and the data definitions. 
We also observe something more troubling: missing symbols. None of $k_c$, 
$h_p$, $h_b$, or $\tau_c$ have units associated with them nor are they defined 
in the table of symbols despite their presence in the equations and data 
definitions for $h_g$ and $h_c$.

As this is a toy example, it may seem contrived to have missing information. 
However we ran into exactly these sorts of issues with our real-world case 
studies. In this example, while we can easily identify and rectify omissions,
it highlights a key issue: even in a simple system, knowledge is repeated 
across different sections of a single \sf{}, and critical definitions can be 
overlooked. Once we add more complexity, with multiple \sfs{} and larger 
systems, these issues compound.

Imagine now that we wish to validate the units of both $h_g$ and $h_c$. The 
knowledge required for this validation is currently missing. Suppose we add the 
appropriate symbol definitions and units to the SRS; if we then discover an 
inconsistency in the units due to an error in the defining equations, we would 
need to update multiple sections of the SRS to correct it, increasing the risk 
of further mistakes. If additional \sfs{}, particularly code, have already been 
created based on these definitions, the challenge of maintaining consistency 
becomes even greater. Corrections would now be required across multiple input 
languages (for example, \LaTeX{} and Python), significantly increasing the 
verification burden. Ultimately, these \sfs{} are communicating much of the 
same knowledge, merely formatted for different audiences. In the case of the 
SRS and the source code, the audiences are human stakeholders and computers, 
respectively, but both require a precise definition of $h_g$ and $h_c$.

This small example foreshadows the larger patterns of redundancy that we 
observe in more complex systems. The rest of this chapter 
generalizes from such examples, showing how we systematically identified, 
categorized, and addressed these issues in the development of Drasil.

\section{A (very) brief introduction to our case study systems}

To ground our analysis in practical reality and move beyond abstract 
principles, this thesis draws on a set of detailed case studies. These case 
studies serve not only to illustrate the prevalence and impact of redundant 
knowledge in \sfs{}, but also to provide a concrete basis for systematically 
identifying, comparing, and generalizing patterns of redundancy and knowledge 
organization. By examining systems developed using common artifact templates, 
we are able to empirically motivate the need for the Drasil framework and 
inform its design. 

To simplify the process of identifying redundancies and patterns, we have chosen
several case studies developed using common artifact templates, specifically 
those used by \smithea{} \ds{source?} Also, as mentioned in 
Section~\ref{sec:scope}, we have chosen software systems that follow the 
\mbox{$`input'~\rightarrow~`process'~\rightarrow~`output'$} pattern. These 
systems cover a variety of use cases, to help avoid over-specializing into one 
particular system type. 

The majority of the aforementioned case studies were developed to solve real
problems. The following cards are meant to be used as a high-level reference to 
each case study, providing the general details at a glance. For the specifics 
of each system, all relevant case study artifacts can be found in the GitHub 
repository.

\card{\gb}
{We need to efficiently and correctly predict whether a glass 
slab can withstand a blast under given conditions.}
{SRS, source code.}

\card{\sw}
{Solar water heating systems incorporating phase change 
 material (PCM) use a renewable energy source and provide a novel way of 
 storing energy. A system is needed to investigate the effect of employing PCM
 within a solar water heating tank.}
{SRS, source code.}

\card{\np}
{Solar water heating systems provide a novel way of 
heating water and storing renewable energy. A system is needed to investigate
the heating of water within a solar water heating tank.}
{SRS, source code.}

The NoPCM case study was created as a software family member for the SWHS case
study. It was manually written, removing all references to PCM and thus 
remodeling the system.

\card{\sp}
{A slope of geological mass, composed of soil and rock 
 and sometimes water, is subject to the influence of gravity on the mass. 
 This can cause instability in the form of soil or rock movement which can
 be hazardous. A system is needed to evaluate the factor of safety of 
 a slope's slip surface and identify the critical slip surface of the slope, 
 as well as the interslice normal force and shear force along the critical 
 slip surface.}
{SRS, source code.}

\card{\pr}
{A system is needed to efficiently and correctly predict
 the landing position of a projectile.}
{SRS, source code.}

The Projectile case study, was the first example of a system 
created solely in Drasil, i.e. we did not have a manually created version to 
compare and contrast with through development. As such, it will not be 
referenced often since it did not inform Drasil's design or development until 
much further in our process. The Projectile case study was created post-facto 
to provide a simple, understandable example for a general audience as it 
requires, at most, a high-school level understanding of physics. 

\card{\gp}
{Many video games need physics libraries that simulate 
 objects acting under various physical conditions, while simultaneously being 
 fast and efficient enough to work in soft real-time during the game. 
 Developing a physics library from scratch takes a long period of time and is 
 very costly, presenting barriers of entry which make it difficult for game 
 developers to include physics in their products.}
{SRS, source code.}

After carefully selecting our case studies, we went about a practical approach
to find and remove redundancies. The first step was to break down each artifact
type and understand exactly what they are trying to convey.


\section{Breaking down \sfs}
\label{sec:breakdown}

To meaningfully address redundancy in software artifacts, it is not enough to 
simply observe that repetition exists; we must also understand the purpose, 
content, and intended audience of each artifact. Building on the foundation 
provided by our case studies, this section systematically examines the 
structure and role of each major \sf{} in our process. By breaking down these 
artifacts, we aim to reveal both their commonalities and their differences, 
setting the stage for identifying patterns of knowledge organization and 
redundancy that inform the design of Drasil.

As noted earlier, for our approach to work we must understand exactly what each
of our artifacts are trying to say and to whom.\footnote{Refer to 
Section~\ref{sec:sfs} for a general summary of \sfs{}.} By selecting our case 
studies from those developed using common artifact templates, we have given 
ourselves a head start on that process, however, there is still much work to be 
done.

The following subsections present a brief sampling of our process of breaking 
down \sfs{}, acknowledging that a comprehensive overview would be excessively 
lengthy.

\subsection{SRS}
\label{sec:breakdown:srs}

To start, we look at the Software Requirements Specification (SRS). The SRS
(or some incarnation of it) is one of the most important artifacts for any
software project as it specifies what problem the software is trying to solve.
There are many ways to state this problem, and the template from \smithea{} has 
given us a strong starting point. Figure~\ref{fig:SRSToC} shows the table of 
contents for an SRS using the \smithea{} template.

\fig{
  \begin{center}
\footnotesize
\begin{enumerate}[nosep, label*=\arabic*.]
\item Reference Material
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Table of Units
  \item Table of Symbols
  \item Abbreviations and Acronyms
\end{enumerate}
\item Introduction
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Purpose of Document
  \item Scope of Requirements
  \item Characteristics of Intended Reader
  \item Organization of Document
\end{enumerate}
\item Stakeholders
\begin{enumerate}[nosep, label*=\arabic*.]
  \item The Customer
  \item The Client
\end{enumerate}
\item General System Description
\begin{enumerate}[nosep, label*=\arabic*.]
  \item System Context
  \item User Characteristics
  \item System Constraints
\end{enumerate}
\item Specific System Description
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Problem Description
\begin{enumerate}[nosep, label*=\arabic*.]
    \item Physical System Description
    \item Goal Statements
\end{enumerate}
  \item Solution Characteristics Specification
\begin{enumerate}[nosep, label*=\arabic*.]
    \item Assumptions
    \item Theoretical Models
    \item General Definitions
    \item Data Definitions
    \item Instance Models
    \item Data Constraints
    \item Properties of a Correct Solution
\end{enumerate}
\end{enumerate}
\item Requirements
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Functional Requirements
  \item Non-Functional Requirements         
\end{enumerate}
\item Likely Changes
\item Unlikely Changes
\item Traceability Matrices and Graphs
\item Values of Auxiliary Constants
\item References
\item Appendix
\end{enumerate}
  \end{center}
}{The Table of Contents from the (\ds{expanded?}) \smithea{} 
template}{fig:SRSToC}

With the structure of the document in mind, let us look at several of our case
studies' SRS documents to get a deeper understanding of what each section truly
represents. Figure~\ref{fig:csRefSecs} shows the reference section of the SRS 
for \gb. Each of the case studies' SRS contains a similar section so for 
brevity we will omit the others here, but they can be found in the GitHub 
repository. 
% Provide a link / ref to relevant appendix
We will look into the case studies in more detail later \ds{will we actually? 
depends on length of chapter}, for now we will try to 
ignore any superficial differences (spelling, grammar, phrasing, etc.) in each 
of them while we look for commonality. We are also trying to determine how the 
non-superficial differences relate to the document template, general problem 
domain, and specific system information.

\fig{
\centering
%\emph{Figure showing the Ref Section of one case 
%			study, split into multiple subfigures - case study TBD}
\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gb_SRS_ToU.png}}
\caption{Table of Units Section}
\label{fig:gbrtou}
\end{subfigure}

\end{figure}

\begin{figure}\ContinuedFloat

\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gb_SRS_ToS.png}}
\caption{Table of Symbols (truncated) Section}
\label{fig:gbrtos}
\end{subfigure}


\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gb_SRS_ToAA.png}}
\caption{Table of Abbreviations and Acronyms (truncated) Section}
\label{fig:gbrtoa}
\end{subfigure}
}
{The reference sections of \gb}
{fig:csRefSecs}

Looking at the (truncated for space) Table of Symbols, Table of Units, and 
Table of Abbreviations and Acronyms sections (Figure~\ref{fig:csRefSecs}) we 
can see that, barring the table values themselves, they are almost identical. 
The Table of Symbols is simply a table of values, akin to a glossary, specific 
to the symbols that appear throughout the rest of the document. For each of 
those symbols, we see the symbol itself, a brief description of what that 
symbol represents, and the units it is measured in, if applicable. Similarly, 
the Table of Units lists the Syst\`eme International d'Unit\'es (SI) Units used 
throughout the document, their descriptions, and the SI name. Finally, the 
table of Abbreviations and Acronyms lists the abbreviations and their full 
forms, which are essentially the symbols and their descriptions for each of the 
abbreviations.

While the reference material section should be fairly self-explanatory as to 
what it contains, other sections and subsections may not be so clear from their 
name alone. For example, it may not be clear offhand of what constitutes a 
theoretical model compared to a data definition or an instance model. One may 
argue that the author of the SRS, particularly if they chose to use the 
\smithea{} template, would need to understand that difference. However, it is 
not clear whether the intended audience would also have such an understanding. 
Who is that audience? Refer to Section~\ref{sec:sfs}, for more details. A brief 
summary is available in Table~\ref{tab:sfsummary}.

Returning to our exercise of breaking down each section of the SRS to determine 
the subtleties of \emph{what} is contained therein\footnote{The breakdown 
details are omitted for brevity and due to their monotonous nature, although 
the overall process is very much akin to the breakdown of the Reference 
Material section.} it should be unsurprising that each section maps to the 
definition provided in the \smithea{} template. However, as noted above, we can 
see distinct differences in the types of information contained in each section. 
Again we find some is boilerplate text meant to give a generic 
(non-system-specific) overview, some is specific to the proposed system, and 
some is in-between: it is specific to the problem domain for the proposed 
system, but not necessarily specific to the system itself.

Observing the contents of an SRS template adhere to said template may seem 
mundane, but it is a necessary step before we can move on to other \sfs{}. 
Without understanding what the SRS template intends to convey it is hard to 
assess weather or not the case study SRS conveys that information. With that in 
mind, we can move on to the MG and source code.

\ds{Current plan for following subsections: Brief description of the \sf{}, 
show an example of similarities within (ex. MG/MIS have a section per module, 
each section is organized the same way, some are filled in, some aren't), then 
follow a requirement through the MG to something in the MIS and finally to 
code. We'll dissect differences between case studies when looking at the 
patterns in Section~\ref{sec:patterns}. This also plants the seeds of "see, 
there's the same info moving from SRS $\rightarrow$ MG $\rightarrow$ MIS** 
$\rightarrow$ Code without stating it 
explicitly, which we can then do in the pattern section.}

\ds{Example to use should be a DD/IM from \gb{}, goes to calculations module 
in the MG, and finally a method in the source code}

\subsection{Module Guide}
\label{sec:breakdown:mg}

The module guide (MG) is a \sf{} that details the architecture of a given 
software system. It holds a number of design decisions around sensibly grouping 
functionalities within the system into modules to fulfill the requirements laid 
out in the SRS. For example, one might have an input/output module for handling 
user input and giving the user feedback through the display (ie. via print 
commands or some other output), or a calculations module that contains all of 
the calculation functions being performed in the normal operation of the given 
software system. The \smithea{} MG template also includes a traceability matrix 
for ease of verifying which requirements are fulfilled by which modules. 
Finally, the MG includes considerations for anticipated or unlikely changes 
that the system may undergo during its lifecycle.

\fig{
\begin{center}
\includegraphics[width=\linewidth]{figures/gb_MG_ToC.png}
\end{center}}
{Table of Contents for \gb{} Module Guide}
{fig:gbrmgtoc}

Figure~\ref{fig:gbrmgtoc} shows the table of contents for the \gb{} case 
study's MG. For the sake of brevity we will omit the other case studies 
here (they can be found at \ds{TODO}). Just as with the SRS we are looking for 
commonality and understanding of what the document is trying to portray to the 
reader. As such we will ignore superficial differences between the MG sections. 
As the MG is a fairly short document we will look at each of the most relevant 
sections as part of this exercise.

Breaking down the MG by section, we can see that the introduction is itself 
completely generic boilerplate explaining the purpose of the MG, the audience, 
and some references to other works that explain why we would make certain 
choices over other (reasonable) ones given the opportunity. There is nothing 
system-specific, nor specific to the given problem domain of the case study.

Following through the table of contents into the ``Anticipated and Unlikely 
Changes" section, we see that again the introductions to this section and 
its subsections are generic boilerplate, however the details of each section 
are not. Both subsections are written in the same way: as a list of labeled 
changes (AC\# for anticipated change, UC\# for unlikely change). This is the 
first place we see both problem-domain and system-specific information 
Interestingly, the Module Hierarchy section follows the same general style: it 
is a list of modules which represent the leaves of the module hierarchy tree 
and each one is labeled (M\#).

\fig{
\begin{center}
\includegraphics[width=\linewidth]{figures/gb_MG_CM.png}
\end{center}}
{Calc Module from the \gb{} Module Guide}
{fig:gbrmgcm}

Skipping ahead to the module decomposition, we find a section heading for each 
Level 1 module in the hierarchy, followed by subsections describing the Level 2 
modules. The former are almost entirely generic boilerplate (for example common 
Level 1 modules include: Hardware-Hiding, Behaviour-Hiding, and 
Software Decision modules), but the latter are problem-domain or system 
specific. An example of a system-specific module is shown in 
Figure~\ref{fig:gbrmgcm}. 

Each module is described by its secrets, services, and what it will be 
implemented by. For example, a given module could be implemented by the 
operating system (OS), the system being described (ex. \gb{}), or a third party 
system/library that will inter-operate with the given system.

Finally we have a traceability matrix and use hierarchy diagram. Both are 
visual representations of how the different modules implement the requirements 
and use each other respectively. The traceability matrix provides a direct and 
obvious link between the SRS and MG, where other connections between the two 
\sfs{} have been implicit until this point. Generally, the next \sf{} would be 
the MIS, however as it is structured so similarly to the MG (one section per 
module, each section organized in a very similar way, a repeated use hierarchy, 
etc) we will skip it for brevity. The MIS includes novel system-specific, 
implementation-level information denoting the interfaces between modules, but 
for our current exercise does not provide any revelations beyond that of the MG.

The MG gives us a very clear picture of \textit{decisions} made by the system 
designers, as opposed to the knowledge of the system domain, problem being 
solved, and requirements of an acceptable solution provided in the SRS. The MG 
provides platform and implementation-specific decisions, which will eventually 
be translated into implementation details in the source code. With that in 
mind, let us move on to the source code.

\subsection{Source Code}
\label{sec:breakdown:code}

The source code is arguably the most important \sf{} in any given software 
system since it serves as the set of instructions that a computer executes in 
order to solve the given problem. With only the other \sfs{} and without the 
source code, we would have a very well defined problem and acceptance criteria 
for a possible solution, but would never actually solve the problem.

\fig{
\begin{center}

\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child 
      anchor)\forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=25pt},
  }
[/src/Python
  [Calc.py]
  [Constants.py]
  [ContoursADT.py]
  [Control.py]
  [Exceptions.py]
  [FunctADT.py]
  [GlassTypeADT.py]
  [Input.py]
  [LoadASTM.py]
  [Output.py]
  [SeqServices.py]
  [ThicknessADT.py]
]
\end{forest}

\end{center}
}
{Python source code directory structure for \gb{}}
{fig:gbsrcstruct}

As the source code is the executable set of instructions, one would expect it 
to be almost entirely system and problem-domain specific with very little 
boilerplate. Looking into the source of our case studies, we find this to be 
mostly true barring the most generic of library use (ex. 
\code{C}{stdio} in C).

Returning to our example of the MG from \gb{} (Figure~\ref{fig:gbrmgtoc}) and 
comparing it to the python source code structure shown in 
Figure~\ref{fig:gbsrcstruct} we can see that the source code follows almost 
identically in structure to the module decomposition. The only difference being 
the existence of an exceptions module defining the different types of
exceptions that may be thrown by the other modules. While this can be 
considered a fairly trivial difference, likely made for ease of maintenance, 
readability, and extensibility, it highlights that the two \sfs{} are out of 
sync. We speculate this difference was caused by a change made during the 
implementation phase, wherein the MG was not updated to reflect the addition of 
an exceptions module.

\fig{
\lstinputlisting[language=Python, firstline=5, 
lastline=61, firstnumber=5]{code/Calc.py}
}{Source code of the Calc.py module for \gb{}}{fig:gbrsrccalc}

Let us look deeper into the code for one specific module, for example the Calc 
module introduced in the MG~(Figure~\ref{fig:gbrmgcm}). The source code for 
said module can be found in Figure~\ref{fig:gbrsrccalc}. In the source code we 
see a number of calculation functions, including those that calculate the 
probability of glass breakage, demand (also known as \emph{load} or $q$), and 
capacity (also known as \emph{load resistance} or $LR$) as outlined in the 
\emph{secrets} section of the Calc module definition in the MG. We also see a 
number of intermediary calculation functions required to calculate these values 
(for example \code{python}|calc_NFL| and its dependencies).

The source code provides clear instructions to the machine on how to calculate 
each of these values and their intermediaries; it provides the actionable steps 
to solve the given problem. When we compare the code with relevant sections of 
the SRS, specifically the Data Definitions (DDs) for each term, we can see a 
very obvious transformation from one form to the other; the symbol used by the 
DD is the (partial) name of the function in the source code and the equation 
from the DD is calculated within the source code. This is one of many patterns 
we see across our \sfs{} within each case study.

\section{Identifying Repetitive Redundancy}
\label{sec:patterns}

From the examples in Section~\ref{sec:breakdown}, we can see a number of simple 
patterns emerging with respect to organization and information repetition 
within a case study. Upon applying our process to all of the case studies and 
adopting a broader perspective, numerous instances emerge where patterns 
transcend individual case studies and remain universally applicable. Several of 
these patterns should be unsurprising, as they relate to the template of a 
particular \sf{}. It is interesting, however, that patterns of information 
organization crop up within a given \sf{} in multiple places, containing 
distinct information.

Returning to our example from Section~\ref{sec:breakdown:srs}, looking only at 
the reference section of our SRS template, we have already found three 
subsections that contain the majority of their information in the same 
organizational structure: a table defining terms with respect to their symbolic 
representation and general information relevant to those terms. Additionally, 
we can see that the Table of Units and Table of Symbols have an introductory 
blurb preceding the tables themselves, whereas the Table of Abbreviations and 
Acronyms does not. Inspecting across case studies, we observe that the 
introduction to the Table of Units is nothing more than boilerplate text 
dropped into each case study verbatim; it is completely generic and applicable 
to \emph{any} software system using SI units. The introduction to the Table of 
Symbols also appears to be boilerplate across several examples, however, it 
does have minor variations which we can see by comparing 
Figure~\ref{fig:gbrtos} to Figure~\ref{fig:gptos} (\gb{} compared to \gp{}). 
These variations reveal the obvious: the variability between systems is greater 
than simply a difference in choice of symbols, and so there is some 
system-specific knowledge being encoded. While we can intuitively infer this 
conclusion based solely on each system addressing a different problem, our 
observation of the (structural) patterns within this SRS section confirms it.

\fig{
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gp_SRS_ToS.png}}
}
{Table of Symbols (truncated) Section from \gp{}}
{fig:gptos}

The reference section of the SRS provides a lot of knowledge in a very 
straightforward and organized manner. The basic units provided in the table of 
units give a prime example of fundamental, global knowledge shared across 
domains. Nearly any system involving physical quantities will use one or more 
of these units. On the other hand, the table of symbols provides 
system/problem-domain specific knowledge that will not be useful across 
unrelated domains. For example, the stress distribution factor $J$ from \gb{} 
may appear in several related problems, but would be unlikely to be seen in 
something like SWHS, NoPCM, or Projectile. Finally, acronyms are very 
context-dependent. They are often specific to a given domain and, without a 
coinciding definition, it can be very difficult for even the target audience to 
understand what they refer to. Within one domain, there may be several acronyms 
that look identical, but mean different things, for example: PM can refer to a 
Product Manager, Project Manager, Program Manager, Portfolio Manager, etc.

By continuing to breakdown the SRS and other \sfs{}, we are able to find many 
more patterns of knowledge repetition. For example, we see the same concept 
being introduced in multiple areas within a single artifact and across 
artifacts in a project.
\fig{
\centering
\includegraphics[width=\textwidth]{figures/gb_dd_q.png}}
{Data Definition for Dimensionless Load ($\hat{q}$) from \gb{} SRS}
{fig:gbrddq}
Figure~\ref{fig:gbrddq} shows the data definition for $\hat{q}$ in \gb{}. That 
same term was previously defined with fewer details in the table of symbols 
(omitted here for brevity), as well as showing up implicitly or in passing 
in the MG (Figure~\ref{fig:gbrmgcm} and the \emph{loadASTM} module 
respectively), and implemented in the Source Code (Figure~\ref{fig:gbrsrccalc} 
lines 11-14). It should be noted that the SRS contains many references to 
$\hat{q}$, such as in the data definitions of the Stress Distribution Factor 
($J$) and Non-Factored Load ($NFL$). There are also implied references through 
intermediate calculations, for example the Calculation of Capacity ($LR$) is 
defined in terms of $NFL$ which relies on $\hat{q}$.

Although the full definition of $\hat{q}$ is initially provided 
for a human audience only once, it is necessary to reference it in different 
ways for different audiences. Each audience is expected to grasp the symbol's 
meaning within their given context or consult other \sfs{} for more 
comprehensive understanding. When reading the SRS, the data definitions and 
other reference materials play a crucial role in swiftly comprehending the 
complete definition of $\hat{q}$ in relation to the system's inputs, outputs, 
functional requirements, and acceptance criteria.

The MG, on the other hand, briefly mentions $\hat{q}$ when defining the 
responsibilities of both the \emph{loadASTM} and \emph{Calc} modules (the 
former being responsible for loading values from a file, and the latter 
utilizing those values for calculations), whereas the source code provides a 
highly detailed definition to ensure accurate execution of the relevant 
calculation(s).

The varying level of detail across the \sfs{} should not come as a surprise 
since each \sf{} targets a different audience and their specific needs at 
various stages of the software development process. Although the level of 
verbosity may differ, the core information remains consistent: the authors are 
consistently referring to the definition of $\hat{q}$ via its symbolic 
representation, regardless of the level of detail incorporated. The goal is to 
convey relevant aspects of knowledge of a given term, while eliding that which 
is deemed superfluous, based on the context and the specific requirements of 
our audience. In other words, the authors only \emph{project} some portion of 
their knowledge of given terms at a given time, depending on their needs 
(precision, brevity, clarity, etc.), the expectations of the audience, and 
contextual relevance. \footnote{We have only referred to the term as $\hat{q}$ 
in this section to emphasize our argument and make a meta-argument that the 
definition is irrelevant to our audience in this example. What matters is the 
symbolic reference, which we share a common understanding of.} The audience, on 
the other hand, engages in \emph{knowledge transformation}, whereby they 
consume the representation (projected knowledge) and transform it into their 
own internal representation, based on their personal knowledge-base.

Relying on common representations, eliding definitions, projecting and 
transforming knowledge are fundamental to the way humans communicate. They are 
readily observable in all forms of communications, whether written or oral, as 
we assign meaning to given sounds and symbols (words) according to the agreed 
upon grammar of a given language and use those words (knowledge projections) to 
simplify communication to a given audience. A context-specific glossary, or 
more generally a dictionary, is a prime example of a knowledge-base that we use 
for communication via knowledge projections and transformations. By maintaining 
a shared vocabulary, we can communicate using the symbolic representations 
(words) instead of requiring terms to be decomposed (defined) to their most 
basic form. However, communication of this sort is still imperfect, due to gaps 
in shared knowledge between participants or misunderstanding of overloaded 
terms. Interpersonal communications can involve nuance and context-dependent 
interpretations, yet they still boil down to knowledge projection on the part of
the communicator and knowledge transformation on the part of the communicatee.
The latter can infer context, or be provided with explicit context, which 
affirms their use of the appropriate knowledge transformations.

Returning to the context of software systems, if we broaden our view from a 
single system, to a software family, we can also find patterns of commonality 
and repeated knowledge across the various \sfs{} of the family members (For 
example the SWHS and NoPCM case studies) as they have been developed to solve 
similar, or in our case nearly identical, problems. Software family members are 
good examples to help determine what types of information or knowledge provided 
in the \sfs{} belong to the system-domain, problem-domain, or are simply 
general (boilerplate).

\fig{
\centering
\includegraphics[width=\textwidth]{figures/swhs_TM1.png}}
{Theoretical Model of conservation of thermal energy found in both the SWHS and 
NoPCM SRS}
{fig:swhstm1}

Looking at SWHS and NoPCM, we can easily find identical theoretical models 
(TMs) as the underlying theory for each system is based on the problem domain 
(see example in Figure~\ref{fig:swhstm1}).
However, when we follow the derivations from the TMs to the Instance Models 
(IMs), we find the resulting equations have changed due to the context of the 
system; the lack of PCM has changed the relevant equations for calculating the 
energy balance on water in the tank as shown in Figure~\ref{fig:swhsnopcmim1}.

\fig{
\centering
%\emph{Figure showing the Ref Section of one case 
%			study, split into multiple subfigures - case study TBD}
\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=0.8\textwidth]{figures/swhs_IM1.png}}
\caption{SWHS Instance Model for Energy Balance on Water}
\label{fig:swhs_im1}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=0.8\textwidth]{figures/nopcm_IM1.png}}
\caption{NoPCM Instance Model for Energy Balance on Water}
\label{fig:nopcm_im1}
\end{subfigure}
}
{Instance Model difference between SWHS and NoPCM }
{fig:swhsnopcmim1}

While the above examples are fairly small and specific, they are indicative of 
a larger, more generalizable, set of patterns of knowledge organization and 
repetition. These patterns are at their core: the use of common knowledge that 
has been projected through some means, and patterns of organization of those 
knowledge projections within \sfs{}. Common knowledge, in this case, refers to 
one of three categories of knowledge: system-specific, domain-specific, or 
common to \sfs{} as a whole. It should also be noted that knowledge projections 
may include the identity projection (ie. the full, unabridged definition) as 
they are dependent on the relevance to the audience of the given \sf{}. 
Regardless, the captured knowledge fundamentally underlies these patterns of 
repetition, and is where we need to focus if we intend to reduce unnecessary 
redundancy.

\section{Organizing knowledge - a fluid approach}
%  **Subsec roadmap:
%    - We see the patterns above, we can generalize a lot of that
%    - Direct repetition (copy-paste) vs indirect repetition (view-changes)
%    require us to pull together knowledge from all artifacts into one place
%    - Some can be derived automatically, the rest must be explicitly stated
%    - We need to create a categorization system (hint at chunks) that is both
%    robust and extensible to cover a wide variety of use cases.
%    - Finally the templates give us structure
%
%  **NOTE: Under the hood section should explain the process of how we 
%determined
%  what we needed to do. What we ended up doing should come in the following
%  section(s) - no 'real' implementation details, only conceptual stuff here.
%
%- Some ``boilerplate" information is actually in the domain of \sf{} writing 
%and 
%thus is so broad, yet relevant to everything we do, that we consider it 
%completely general. Things like stakeholders, characteristics of the intended 
%audience, etc.

Given the knowledge categories and patterns of use across \sfs{} we have seen 
in the previous section, we can generalize knowledge projections by their 
projection functions. \ds{What follows is very rough and may need some 
definitions/tweaking to be more clear, but it makes sense in my head} Identity 
projection functions directly repeat knowledge 
verbatim i.e. $p(K) \equiv K$ for some piece of knowledge $k$ \ds{I think I'll 
remove all of the set notation here, it comes out of nowhere and probably won't 
come up anywhere else or be defined anywhere so it's a bit jarring. I'm leaving 
it in for now because it makes sense to me}. For example a 
copy-and-paste approach would be an identity projection.  It should be noted in 
our case that as long as the projection used contains the full definition and 
context of a piece of knowledge, that projection is considered an identity 
projection regardless of changes to notation (ex. ``$x = y$" vs ``$y = x$") or 
language (ex. ``$x = y$" vs ``$x$ is equal to $y$" vs ``x est \'egal \`a y"). 
\ds{Please correct my French if I'm wrong here}
Non-identity projections require using representations that elide some details 
of the knowledge at hand to make it more palatable for the audience of the 
given \sf{} i.e. $p(K) \subset K$. Similarly, we can have multivariant 
projection functions (whether identity or not) which project knowledge from 
several places into one form, i.e. $p(K,L,M) \subseteq K\cup L\cup M$.

In both cases, we have a knowledge core that is fully defined and then we apply 
a projection function to retrieve the necessary information for our \sf{}. We 
can postulate that organizing our knowledge cores into some type of structure, 
with some assortment of projection functions (ex. Looking up the full 
definition would be done through an identity projection function) will allow 
us to reduce the need for manual duplication and remove unnecessary 
redundancies, as anything we need to include in our \sfs{} can be retrieved 
from a given source with a given projection function.

Keeping in mind that our core knowledge is used across all \sfs{} via 
projections, we may naively choose to consolidate all knowledge cores into one 
database. This naive approach works well enough for a limited set of examples, 
but it quickly becomes apparent (refer to the PM example from 
Section~\ref{sec:patterns}) that context is highly important and the sheer 
scope of knowledge to be organized may become unwieldy. After breaking down 
multiple case studies, we believe collecting knowledge cores into categories 
based on their domain(s) is a more easily navigable and maintainable approach. 
This also allows us to keep some context information at a meta-level (ex. 
Physics knowledge would be categorized into a Physics knowledge-base). Then for 
any given system, we would likely only need to reference across a handful of 
contexts (knowledge-bases) relevant to the domain.

There is knowledge fundamental to all \sfs{} as it is contextual to the domain 
of \sf{} writing itself. This kind of meta-knowledge would be useful to have 
readily available in its own knowledge-base. The same could be said for things 
like SI Unit definitions, while they only apply to measuring physical 
properties, we see some domains built off of physics that operate at a higher 
level of assumed understanding (ex. Chemistry abstracts some of the physics 
details, while being directly reliant on them).

Some of the knowledge used in our \sfs{} is derived from other, more 
fundamental, knowledge cores. For example, when using SI units, we may choose 
to use a derived unit (newton, joule, radian, etc.) which 
is a better fit for the application domain of the system being documented. 
While we want to avoid unnecessary redundancy, we can argue that derived units 
are good candidates for acceptable redundancy. For example, if anywhere we use 
\emph{Joules} we replace that with the definition ($J = 
\frac{kg\cdot{}m^2}{s^2}$) we then run into a problem of context and 
complexity. Generally, the audience for a given \sf{} will have an internal 
representation of context-specific knowledge, so even something as 
straightforward as changing the units from $J$ to $\frac{kg\cdot{}m^2}{s^2}$ 
will put unnecessary load on said audience and force them to engage in more 
intensive knowledge transformations, while also potentially making the \sfs{} 
harder to parse for experts in the domain. In these cases, we want to use the 
derived knowledge in place of the core knowledge.

Being able to specify our level of abstraction through the progressive 
application of projection functions eludes to another necessary piece of 
knowledge organization: the projection functions themselves. As we project out 
core knowledge that we know of as otherwise commonly derived concepts (like the 
Joule example above), we should also like to store them. For example, derived 
units may end up in the same context as the SI Units, defined by specific 
projection functions applied to SI Units. Continuing the Joule example, we 
would be applying a projection function across core knowledge related to energy 
and specific SI Units, then calling that projection \emph{Joule} and giving it 
a symbolic representation $J$ that we can refer to later.

We want to take a fluid and practical approach to organizing knowledge, such 
that we can keep domain-related knowledge cores together with useful 
derivations. We want to separate knowledge in unrelated domains, such that it 
is straightforward to look up whatever we need with relative ease. The 
specific implementation for organization will be detailed later (See 
Chapter~\ref{sec:kc}).

\ds{Next section needs to summarize "we need to capture knowledge", "store 
knowledge", "project knowledge", and have the framework to support that across 
multiple \sf{} domains and multiple code langs}
  
\section{Summary - The seeds of Drasil}
%  **Subsec roadmap:
%    -- Summarize the above subsections and lead into next section
%    -- Add relevant information that doesn't quite fit above 
%      and isn't implementation related
%    -- 'Relevant buckshot section'

\ds{Point out what the reader should have learned from this chapter before 
anything else}

Through this chapter, as part of our effort to reduce unnecessary redundancy 
across the software development process, we have taken an approach to breaking 
down \sfs{} to the core knowledge they present and looked for commonalities in 
that knowledge between them. We use several case study systems that fit our 
scope (input $\rightarrow$ process $\rightarrow$ output) as examples to give a 
concrete, applied base to the work.

Generally, we see \sfs{} for a given system have a lot in common, namely they 
require the same core knowledge tailored to a specific audience for each \sf{}. 
This knowledge is organized in a meaningful way, and portions relevant to the 
context of the \sf{} are presented to the audience.

We delved into the idea of knowledge cores and projection functions for 
producing context-relevant pieces of knowledge that are consumable by a given 
audience. We have also explored strategies for organizing that knowledge in a 
practical manner. 

We have determined the three main components necessary for any useful \sf{}: 
knowledge, context (ie. audience), and organizational structure. From here we 
can operationalize each component in a reusable and (relatively) 
redundancy-free manner. This operationalization informs the initial design for 
our framework Drasil which will be covered in depth in Chapter~\ref{c:drasil}. 

Effectively, we want to automate the generation of \sfs{} through applying 
projections to knowledge and presenting it in a given structure. The structure 
of \sfs{} is relatively straightforward to deal with, we can use templates, 
blueprints, or deterministic generation which rely on relatively common 
technologies. The knowledge-capture and projection is much more interesting as 
it relies on some yet-to-be-determined knowledge-capture mechanism that can 
provide us with chunks (borrowing the term from LP) of knowledge that can then 
be fed to projection functions in some context-aware manner.

\ds{Want to work in something about ``our framework needs to be developed with 
consistency in mind, so we take a practical, example-driven (case studies) 
approach to minimize introducing new errors and inconsistencies." Could also 
fit in Drasil section, but I feel like introducing it here would be better.}

%- Everything boils down to knowledge we are trying to convey, audience 
%(context), and organizational structure. The latter can be 
%handled through a number of means (deterministic generation / templates / 
%blueprints). The former is more interesting as it essentially boils down to a 
%particular grouping of information (knowledge-base) and some way to transform 
%or project out what is relevant given a set of characteristics or expectations.