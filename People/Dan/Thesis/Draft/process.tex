\chapter{A look under the hood: \mbox{Our process}}
\label{c:process}

This chapter examines the process by which we developed the Drasil 
framework itself. Rather than describing the general software development 
process that Drasil is intended to support or automate, we turn our attention 
inward to examine the steps, decisions, and rationale that guided its creation. 
Our goal is to provide transparency into the reasoning that shaped Drasil, 
highlighting the challenges encountered and the strategies employed to address 
them. By doing so, we aim to give the reader a clear understanding of how the 
framework's design emerged from a careful analysis of redundancy and knowledge 
organization across software artifacts.

As one of the central motivations for developing Drasil was the observation 
that \sfs{} contain significant, unnecessary redundancy, we begin by motivating 
our approach through concrete examples of redundancy in \sfs{}. We then 
describe how we selected and analyzed a set of representative case studies, 
breaking down their artifacts to identify recurring patterns and organizational 
structures. This analysis leads to a general methodology for knowledge capture 
and projection, which ultimately shaped the core principles of Drasil. 
Throughout, we emphasize the interplay between practical experience and 
theoretical insight, illustrating how each informed our process and contributed 
to the framework's evolution.

\section{A minimal motivating example: HGHC}
\label{sec:hghc-mini}

Before delving into the full complexity of our case studies and artifact 
templates, it is helpful to illustrate the core problem of knowledge redundancy 
with an incredibly minimal, toy example. For this, we use the ``HGHC'' system, 
a simple heat transfer case study included in the repository.

The HGHC example's SRS specifies what problem the software is trying to solve/ 
In this case, it should model heat transfer coefficients in nuclear fuel rods, 
specifically $h_g$ (gap conductance) and $h_c$ (effective heat transfer 
coefficient between clad and coolant). Even in this minimal system, the same 
core knowledge appears in multiple places within the SRS.

To illustrate, consider the following: The SRS defines variables such as $h_g$, 
$h_c$, $\tau_c$ (clad thickness), $h_p$ (initial gap film conductance), $h_b$ 
(initial coolant film conductance), and $k_c$ (clad conductivity). The defining 
equations for $h_g$ and $h_c$ are also given as:
    \[
      h_g = \frac{2k_{c}h_{p}}{2k_{c}+\tau_c h_{p}}
    \]
    \[
      h_c = \frac{2k_{c}h_{b}}{2k_{c}+\tau_c h_{b}}
    \]

We can already see repeated knowledge ($k_c$ and $\tau_c$) in these two 
equations. The redundancy goes even deeper in the full SRS (which can be 
found in Appendix~\ref{appendix:hghc}). There we can see repeated descriptions 
and units for $h_g$ and $h_c$ in the table of symbols and the data definitions. 
We also observe something more troubling: missing symbols. None of $k_c$, 
$h_p$, $h_b$, or $\tau_c$ have units associated with them nor are they defined 
in the table of symbols despite their presence in the equations and data 
definitions for $h_g$ and $h_c$.

As this is a toy example, it may seem contrived to have missing information. 
However we ran into exactly these sorts of issues with our real-world case 
studies. In this example, while we can easily identify and rectify omissions,
it highlights a key issue: even in a simple system, knowledge is repeated 
across different sections of a single \sf{}, and critical definitions can be 
overlooked. Once we add more complexity, with multiple \sfs{} and larger 
systems, these issues compound.

Imagine now that we wish to validate the units of both $h_g$ and $h_c$. The 
knowledge required for this validation is currently missing. Suppose we add the 
appropriate symbol definitions and units to the SRS; if we then discover an 
inconsistency in the units due to an error in the defining equations, we would 
need to update multiple sections of the SRS to correct it, increasing the risk 
of further mistakes. If additional \sfs{}, particularly code, have already been 
created based on these definitions, the challenge of maintaining consistency 
becomes even greater. Corrections would now be required across multiple input 
languages (for example, \LaTeX{} and Python), significantly increasing the 
verification burden. Ultimately, these \sfs{} are communicating much of the 
same knowledge (i.e. the definitions of $h_g$ and $h_c$), merely formatted for 
different audiences. In the case of the SRS and the source code, the audiences 
are human stakeholders and computers, respectively.

This small example foreshadows the larger patterns of redundancy and risks of 
inconsistency that we observe in more complex systems. The rest of this chapter 
generalizes from such examples, showing how we systematically identified, 
categorized, and addressed these issues in the development of Drasil.

\section{A (very) brief introduction to our case study systems}

To ground our analysis in reality and move beyond abstract principles, this 
thesis draws on a set of detailed case studies. These case studies serve not 
only to illustrate the prevalence and impact of redundant knowledge in \sfs{}, 
but also to provide a concrete basis for systematically identifying, comparing, 
and generalizing patterns of redundancy and knowledge organization. By 
examining systems developed using common artifact templates, we are able to 
motivate the need for the Drasil framework and inform its design. 

To simplify the process of identifying redundancies and patterns, we have chosen
several case studies developed using common artifact templates, specifically 
those used by \smithea{} \ds{source?} Also, as previously mentioned in 
Section~\ref{sec:scope}, our case study software systems follow the 
\mbox{$`input'~\rightarrow~`process'~\rightarrow~`output'$} pattern. These 
systems cover a variety of use cases, to help avoid over-specializing into one 
particular system type. 

The majority of the aforementioned case studies were developed to solve real
problems. The following cards are meant to be used as a high-level reference to 
each case study, providing the general details at a glance. For the specifics 
of each system, all relevant case study artifacts can be found in the GitHub 
repository.

\ds{The cards were initially going to say more, but I might convert them into a 
table at this point. Though I like a bit of visual difference.}
%if removing cards, update above sentence

\card{\gb}
{We need to efficiently and correctly predict whether a glass 
slab can withstand a blast under given conditions.}
{SRS, source code.}

\card{\sw}
{Solar water heating systems incorporating phase change 
 material (PCM) use a renewable energy source and provide a novel way of 
 storing energy. A system is needed to investigate the effect of employing PCM
 within a solar water heating tank.}
{SRS, source code.}

\card{\np}
{Solar water heating systems provide a novel way of 
heating water and storing renewable energy. A system is needed to investigate
the heating of water within a solar water heating tank.}
{SRS, source code.}

The NoPCM case study was created as a software family member for the SWHS case
study. It was manually written, removing all references to PCM and thus 
remodeling the system.

\card{\sp}
{A slope of geological mass, composed of soil and rock 
 and sometimes water, is subject to the influence of gravity on the mass. 
 This can cause instability in the form of soil or rock movement which can
 be hazardous. A system is needed to evaluate the factor of safety of 
 a slope's slip surface and identify the critical slip surface of the slope, 
 as well as the interslice normal force and shear force along the critical 
 slip surface.}
{SRS, source code.}

\card{\pr}
{A system is needed to efficiently and correctly predict
 the landing position of a projectile.}
{SRS, source code.}

The Projectile case study, was the first example of a system 
created solely in Drasil, i.e. we did not have a manually created version to 
compare and contrast with through development. As such, it will not be 
referenced often since it did not inform Drasil's design or development until 
much further in our process. The Projectile case study was created post-facto 
to provide a simple, understandable example for a general audience as it 
requires, at most, a high-school level understanding of physics. 

\card{\gp}
{Many video games need physics libraries that simulate 
 objects acting under various physical conditions, while simultaneously being 
 fast and efficient enough to work in soft real-time during the game. 
 Developing a physics library from scratch takes a long period of time and is 
 very costly, presenting barriers of entry which make it difficult for game 
 developers to include physics in their products.}
{SRS, source code.}

After carefully selecting our case studies, we went about a practical approach
to find and remove redundancies. The first step was to break down each artifact
type and understand exactly what they are trying to convey.


\section{Breaking down \sfs}
\label{sec:breakdown}

To meaningfully address redundancy in software artifacts, we must understand 
the purpose, content, and audience of each artifact\textemdash{}not just 
observe repetition. Building on the foundation provided by our case studies, 
this section examines the structure and role of each major \sf{}\footnote{For 
brevity, we focus our analysis on the SRS, Module Guide, and Source Code, as 
these artifacts are both representative and sufficiently distinct to illustrate 
the patterns of redundancy and knowledge organization central to our argument.} 
in our process. By breaking down these artifacts, we aim to reveal both their 
commonalities and their differences, setting the stage for identifying patterns 
of knowledge organization and redundancy that inform the design of Drasil.

As noted earlier, for our approach to work we must understand exactly what each
of our \sfs{} are trying to say and to whom.\footnote{Refer to 
Section~\ref{sec:sfs} for a general summary of \sfs{}.} By selecting our case 
studies from those developed using common artifact templates, we have given 
ourselves a head start on that process. The following subsections present a 
brief sampling of our process of breaking down \sfs{}, using the \gb{} case 
study as a running, motivating example to ground the analysis, acknowledging 
that a comprehensive overview would be excessively lengthy.

\subsection{SRS}
\label{sec:breakdown:srs}

To start, we look at the Software Requirements Specification (SRS). The SRS
(or some incarnation of it) is one of the most important artifacts for any
software project as it specifies what problem the software is trying to solve.
There are many ways to state this problem, and the template from \smithea{} has 
given us a strong starting point. Figure~\ref{fig:SRSToC} shows the table of 
contents for an SRS using the \smithea{} template.

\fig{
  \begin{center}
\footnotesize
\begin{enumerate}[nosep, label*=\arabic*.]
\item Reference Material
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Table of Units
  \item Table of Symbols
  \item Abbreviations and Acronyms
\end{enumerate}
\item Introduction
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Purpose of Document
  \item Scope of Requirements
  \item Characteristics of Intended Reader
  \item Organization of Document
\end{enumerate}
\item Stakeholders
\begin{enumerate}[nosep, label*=\arabic*.]
  \item The Customer
  \item The Client
\end{enumerate}
\item General System Description
\begin{enumerate}[nosep, label*=\arabic*.]
  \item System Context
  \item User Characteristics
  \item System Constraints
\end{enumerate}
\item Specific System Description
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Problem Description
\begin{enumerate}[nosep, label*=\arabic*.]
    \item Physical System Description
    \item Goal Statements
\end{enumerate}
  \item Solution Characteristics Specification
\begin{enumerate}[nosep, label*=\arabic*.]
    \item Assumptions
    \item Theoretical Models
    \item General Definitions
    \item Data Definitions
    \item Instance Models
    \item Data Constraints
    \item Properties of a Correct Solution
\end{enumerate}
\end{enumerate}
\item Requirements
\begin{enumerate}[nosep, label*=\arabic*.]
  \item Functional Requirements
  \item Non-Functional Requirements         
\end{enumerate}
\item Likely Changes
\item Unlikely Changes
\item Traceability Matrices and Graphs
\item Values of Auxiliary Constants
\item References
\item Appendix
\end{enumerate}
  \end{center}
}{The Table of Contents from the expanded \smithea{} template}{fig:SRSToC}

With the structure of the document in mind, let us look at several of our case
studies' SRS documents to get a deeper understanding of what each section truly
represents. Figure~\ref{fig:csRefSecs} shows the reference section of the SRS 
for \gb. Each of the case studies' SRS contains a similar section so for 
brevity we will omit the others here, but they can be found in the GitHub 
repository. We will try to ignore any superficial differences (spelling, 
grammar, phrasing, etc.) in each of the case studies while we look for 
commonality. We are also trying to determine how the non-superficial 
differences relate to the document template, general problem domain, and 
specific system information.

\ds{The figure with subfigures might be a hassle to keep co-located, might 
split it up into 3 separate figures and just refer to them collectively}

\fig{
\centering
\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gb_SRS_ToU.png}}
\caption{Table of Units Section}
\label{fig:gbrtou}
\end{subfigure}

\end{figure}

\begin{figure}\ContinuedFloat

\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gb_SRS_ToS.png}}
\caption{Table of Symbols (truncated) Section}
\label{fig:gbrtos}
\end{subfigure}


\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gb_SRS_ToAA.png}}
\caption{Table of Abbreviations and Acronyms (truncated) Section}
\label{fig:gbrtoa}
\end{subfigure}
}
{The reference sections of \gb}
{fig:csRefSecs}

Looking at the (truncated for space) Table of Symbols, Table of Units, and 
Table of Abbreviations and Acronyms sections (Figure~\ref{fig:csRefSecs}) we 
can see that, barring the table values themselves, they are almost identical. 
The Table of Symbols is simply a table of values, akin to a glossary, specific 
to the symbols that appear throughout the rest of the document. For each of 
those symbols, we see the symbol itself, a brief description of what that 
symbol represents, and the units it is measured in, if applicable. Similarly, 
the Table of Units lists the Syst\`eme International d'Unit\'es (SI) Units used 
throughout the document, their descriptions, and the SI name. Finally, the 
table of Abbreviations and Acronyms lists the abbreviations and their full 
forms, which are essentially the symbols and their descriptions for each of the 
abbreviations.

While the reference material section should be fairly self-explanatory as to 
what it contains, other sections and subsections may not be so clear from their 
name alone. For example, it may not be clear offhand of what constitutes a 
theoretical model compared to a data definition or an instance model. One may 
argue that the author of the SRS, particularly if they chose to use the 
\smithea{} template, would need to understand that difference. However, it is 
not clear whether the intended audience would also have such an understanding. 
Who is that audience? Refer to Section~\ref{sec:sfs}, for more details. A brief 
summary is available in Table~\ref{tab:sfsummary}.

Returning to our exercise of breaking down each section of the SRS to determine 
the subtleties of \emph{what} is contained therein\footnote{The breakdown 
details are omitted for brevity and due to their monotonous nature, although 
the overall process is very much akin to the breakdown of the Reference 
Material section.} it should be unsurprising that each section maps to the 
definition provided in the \smithea{} template. However, as noted above, we can 
see distinct differences in the types of information contained in each section. 
Again we find some is boilerplate text meant to give a generic 
(non-system-specific) overview, some is specific to the proposed system, and 
some is in-between: it is specific to the problem domain for the proposed 
system, but not necessarily specific to the system itself. For example, many 
theoretical models would fall into that in-between category\textemdash{}they 
represent theories from their problem domain (ex. Physics) that are used to 
derive the system-specific knowledge.

Observing the contents of an SRS template adhere to said template may seem 
mundane, but it is a necessary step before we can move on to other \sfs{}. 
Without understanding what the SRS template intends to convey it is hard to 
assess whether or not the case study SRS conveys that information. With that in 
mind, we can move on to the MG and source code.

\subsection{Module Guide}
\label{sec:breakdown:mg}

The module guide (MG) is a \sf{} that details the architecture of a given 
software system. It holds a number of design decisions around sensibly grouping 
functionalities within the system into modules to fulfill the requirements laid 
out in the SRS. For example, one might have an input/output module for handling 
user input and giving the user feedback through the display (ie. via print 
commands or some other output), or a calculations module that contains all of 
the calculation functions being performed in the normal operation of the given 
software system. The \smithea{} MG template also includes a traceability matrix 
for ease of verifying which requirements are fulfilled by which modules. 
Finally, the MG includes considerations for anticipated or unlikely changes 
that the system may undergo during its lifecycle.

\fig{
\begin{center}
\includegraphics[width=\linewidth]{figures/gb_MG_ToC.png}
\end{center}}
{Table of Contents for \gb{} Module Guide}
{fig:gbrmgtoc}

Figure~\ref{fig:gbrmgtoc} shows the table of contents for the \gb{} case 
study's MG. Again, for the sake of brevity we will omit the other case studies 
here, even though we engaged in the same breakdown process for each of them. 
Just as with the SRS we are looking for commonality and understanding of 
what the document is trying to portray to the reader. As such we will ignore 
superficial differences between the MG sections. As the MG is a fairly short 
document we will look at each of the most relevant sections as part of this 
exercise.

Breaking down the MG by section, we can see that the introduction is itself 
completely generic boilerplate explaining the purpose of the MG, the audience, 
and some references to other works that explain why we would make certain 
choices over other (reasonable) ones given the opportunity. There is nothing 
system-specific, nor specific to the given problem domain of the case study.

Following through the table of contents into the ``Anticipated and Unlikely 
Changes" section, we see that again the introductions to this section and 
its subsections are generic boilerplate, however the details of each section 
are not. Both subsections are written in the same way: as a list of labeled 
changes (AC\# for anticipated change, UC\# for unlikely change). This is the 
first place we see both problem-domain and system-specific information 
Interestingly, the Module Hierarchy section follows the same general style: it 
is a list of modules which represent the leaves of the module hierarchy tree 
and each one is labeled (M\#).

\fig{
\begin{center}
\includegraphics[width=\linewidth]{figures/gb_MG_CM.png}
\end{center}}
{Calc Module from the \gb{} Module Guide}
{fig:gbrmgcm}

Skipping ahead to the module decomposition, we find a section heading for each 
Level 1 module in the hierarchy, followed by subsections describing the Level 2 
modules. The former are almost entirely generic boilerplate (for example common 
Level 1 modules include: Hardware-Hiding, Behaviour-Hiding, and 
Software Decision modules), but the latter are problem-domain or system 
specific. An example of a system-specific module is shown in 
Figure~\ref{fig:gbrmgcm}. 

Each module is described by its secrets, services, and what it will be 
implemented by. For example, a given module could be implemented by the 
operating system (OS), the system being described (ex. \gb{}), or a third party 
system/library that will inter-operate with the given system.

Finally we have a traceability matrix and use hierarchy diagram. Both are 
visual representations of how the different modules implement the requirements 
and use each other respectively. The traceability matrix provides a direct and 
obvious link between the SRS and MG, where other connections between the two 
\sfs{} have been implicit until this point. Generally, the next \sf{} would be 
the MIS, however as it is structured so similarly to the MG (one section per 
module, each section organized in a very similar way, a repeated use hierarchy, 
etc) we will skip it for brevity. The MIS includes novel system-specific, 
implementation-level information denoting the interfaces between modules, but 
for our current exercise does not provide any revelations beyond that of the MG.

The MG gives us a very clear picture of \textit{decisions} made by the system 
designers, as opposed to the knowledge of the system domain, problem being 
solved, and requirements of an acceptable solution provided in the SRS. The MG 
provides platform and implementation-specific decisions, which will eventually 
be translated into implementation details in the source code. With that in 
mind, let us move on to dissecting the source code.

\subsection{Source Code}
\label{sec:breakdown:code}

The source code is arguably the most important \sf{} in any given software 
system since it serves as the set of instructions that a computer executes in 
order to solve the given problem. With only the other \sfs{} and without the 
source code, we would have a very well defined problem and acceptance criteria 
for a possible solution, but would never actually solve the problem.

\fig{
\begin{center}

\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child 
      anchor)\forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=25pt},
  }
[/src/Python
  [Calc.py]
  [Constants.py]
  [ContoursADT.py]
  [Control.py]
  [Exceptions.py]
  [FunctADT.py]
  [GlassTypeADT.py]
  [Input.py]
  [LoadASTM.py]
  [Output.py]
  [SeqServices.py]
  [ThicknessADT.py]
]
\end{forest}

\end{center}
}
{Python source code directory structure for \gb{}}
{fig:gbsrcstruct}

As the source code is the executable set of instructions, one would expect it 
to be almost entirely system and problem-domain specific with very little 
boilerplate. Looking into the source of our case studies, we find this to be 
mostly true barring the most generic of library use (ex. 
\code{C}{stdio} in C).

Returning to our example of the MG from \gb{} (Figure~\ref{fig:gbrmgtoc}) and 
comparing it to the python source code structure shown in 
Figure~\ref{fig:gbsrcstruct} we can see that the source code follows almost 
identically in structure to the module decomposition. The only difference being 
the existence of an exceptions module defining the different types of
exceptions that may be thrown by the other modules. While this can be 
considered a fairly trivial difference, likely made for ease of maintenance, 
readability, and extensibility, it highlights that \textbf{the two \sfs{} are 
out of sync}. We speculate this difference was caused by a change made during 
the implementation phase, wherein the MG was not updated to reflect the 
addition of an exceptions module.

\fig{
\lstinputlisting[language=Python, firstline=5, 
lastline=61, firstnumber=5]{code/Calc.py}
}{Source code of the Calc.py module for \gb{}}{fig:gbrsrccalc}

Let us look deeper into the code for one specific module, for example the Calc 
module introduced in the MG~(Figure~\ref{fig:gbrmgcm}). The source code for 
said module can be found in Figure~\ref{fig:gbrsrccalc}. In the source code we 
see a number of calculation functions, including those that calculate the 
probability of glass breakage, demand (also known as \emph{load} or $q$), and 
capacity (also known as \emph{load resistance} or $LR$) as outlined in the 
\emph{secrets} section of the Calc module definition in the MG. We also see a 
number of intermediary calculation functions required to calculate these values 
(for example \code{python}|calc_NFL| and its dependencies).

The source code provides clear instructions to the machine on how to calculate 
each of these values and their intermediaries; it provides the actionable steps 
to solve the given problem. When we compare the code with relevant sections of 
the SRS, specifically the Data Definitions (DDs) for each term, we can see a 
very obvious transformation from one form to the other; the symbol used by the 
DD is the (partial) name of the function in the source code and the equation 
from the DD is calculated within the source code. This is one of many patterns 
we see across our \sfs{} within each case study.

\subsection{Wrapping up}

A close examination of each major software artifact reveals that, while their 
purposes and audiences differ, they are all fundamentally built upon the same 
core knowledge. Redundancy is introduced both within and across these 
artifacts, often as a result of tailoring the same information for different 
contexts or audiences. Structural similarities emerge across artifacts, 
highlighting opportunities for systematic knowledge capture and reuse. At the 
same time, inconsistencies and omissions can easily arise when artifacts are 
maintained separately, underscoring the risks inherent in manual, disconnected 
processes. Understanding the intent, content, and audience of each artifact is 
therefore essential for identifying patterns of redundancy and for informing 
the design of tools that aim to improve consistency and reduce unnecessary 
repetition.

By systematically dissecting each artifact type, we not only reveal the sources 
and forms of redundancy, but also clarify the requirements for any framework 
(Drasil) that aims to automate or improve the generation of these artifacts. 
This analysis directly informs the patterns and organizational strategies we 
discuss in the following sections.

\section{Identifying Repetitive Redundancy}
\label{sec:patterns}

From the examples in Sections~\ref{sec:hghc-mini}~and~\ref{sec:breakdown}, we 
can see a number of simple patterns emerging with respect to organization and 
information repetition within a case study. Upon applying our process to all of 
the case studies and adopting a broader perspective, numerous instances emerge 
where patterns transcend individual case studies and remain universally 
applicable. Several of these patterns should be unsurprising, as they relate to 
the template of a particular \sf{}. It is interesting, however, that patterns 
of information organization crop up within a given \sf{} in multiple places, 
containing distinct information.

Returning to our example from Section~\ref{sec:breakdown:srs}, looking only at 
the reference section of our SRS template, we have already found three 
subsections that contain the majority of their information in the same 
organizational structure: a table defining terms with respect to their symbolic 
representation and general information relevant to those terms. Additionally, 
we can see that the Table of Units and Table of Symbols have an introductory 
blurb preceding the tables themselves, whereas the Table of Abbreviations and 
Acronyms does not. Inspecting across case studies, we observe that the 
introduction to the Table of Units is nothing more than boilerplate text 
dropped into each case study verbatim; it is completely generic and applicable 
to \emph{any} software system using SI units. The introduction to the Table of 
Symbols also appears to be boilerplate across several examples, however, it 
does have minor variations which we can see by comparing 
Figure~\ref{fig:gbrtos} to Figure~\ref{fig:gptos} (\gb{} compared to \gp{}). 
These variations reveal the obvious: the variability between systems is greater 
than simply a difference in choice of symbols, and so there is some 
system-specific knowledge being encoded. While we can intuitively infer this 
conclusion based solely on each system addressing a different problem, our 
observation of the (structural) patterns within this SRS section confirms it.

\fig{
\centering
\fbox{\includegraphics[width=\textwidth]{figures/gp_SRS_ToS.png}}
}
{Table of Symbols (truncated) Section from \gp{}}
{fig:gptos}

The reference section of the SRS provides a lot of knowledge in a very 
straightforward and organized manner. The basic units provided in the table of 
units give a prime example of fundamental, global knowledge shared across 
domains. Nearly any system involving physical quantities will use one or more 
of these units. On the other hand, the table of symbols provides 
system/problem-domain specific knowledge that will not be useful across 
unrelated domains. For example, the stress distribution factor $J$ from \gb{} 
may appear in several related problems, but would be unlikely to be seen in 
something like SWHS, NoPCM, or Projectile. Finally, acronyms are very 
context-dependent. They are often specific to a given domain and, without a 
coinciding definition, it can be very difficult for even the target audience to 
understand what they refer to. Within one domain, there may be several acronyms 
that look identical, but mean different things, for example: PM can refer to a 
Product Manager, Project Manager, Program Manager, Portfolio Manager, etc.

By continuing to breakdown the SRS and other \sfs{}, we are able to find many 
more patterns of knowledge repetition. For example, we see the same concept 
being introduced in multiple areas within a single artifact and across 
artifacts in a project.
\fig{
\centering
\includegraphics[width=\textwidth]{figures/gb_dd_q.png}}
{Data Definition for Dimensionless Load ($\hat{q}$) from \gb{} SRS}
{fig:gbrddq}
Figure~\ref{fig:gbrddq} shows the data definition for $\hat{q}$ in \gb{}. That 
same term was previously defined with fewer details in the table of symbols 
(omitted here for brevity), as well as showing up implicitly or in passing 
in the MG (Figure~\ref{fig:gbrmgcm} and the \emph{loadASTM} module 
respectively), and implemented in the Source Code (Figure~\ref{fig:gbrsrccalc} 
lines 11-14). It should be noted that the SRS contains many references to 
$\hat{q}$, such as in the data definitions of the Stress Distribution Factor 
($J$) and Non-Factored Load ($NFL$). There are also implied references through 
intermediate calculations, for example the Calculation of Capacity ($LR$) is 
defined in terms of $NFL$ which relies on $\hat{q}$.

Although the full definition of $\hat{q}$ is initially provided 
for a human audience only once, it is necessary to reference it in different 
ways for different audiences. Each audience is expected to grasp the symbol's 
meaning within their given context or consult other \sfs{} for more 
comprehensive understanding. When reading the SRS, the data definitions and 
other reference materials play a crucial role in swiftly comprehending the 
complete definition of $\hat{q}$ in relation to the system's inputs, outputs, 
functional requirements, and acceptance criteria.

The MG, on the other hand, briefly mentions $\hat{q}$ when defining the 
responsibilities of both the \emph{loadASTM} and \emph{Calc} modules (the 
former being responsible for loading values from a file, and the latter 
utilizing those values for calculations), whereas the source code provides a 
highly detailed definition to ensure accurate execution of the relevant 
calculation(s).

The varying level of detail across the \sfs{} should not come as a surprise 
since each \sf{} targets a different audience and their specific needs at 
various stages of the software development process. Although the level of 
verbosity may differ, the core information remains consistent: the authors are 
consistently referring to the definition of $\hat{q}$ via its symbolic 
representation, regardless of the level of detail incorporated. The goal is to 
convey relevant aspects of knowledge of a given term, while eliding that which 
is deemed superfluous, based on the context and the specific requirements of 
our audience. In other words, the authors only \emph{project} some portion of 
their knowledge of given terms at a given time, depending on their needs 
(precision, brevity, clarity, etc.), the expectations of the audience, and 
contextual relevance. \footnote{We have only referred to the term as $\hat{q}$ 
in this section to emphasize our argument and make a meta-argument that the 
definition is irrelevant to our audience in this example. What matters is the 
symbolic reference, which we share a common understanding of.} The audience, on 
the other hand, engages in \emph{knowledge transformation}, whereby they 
consume the representation (projected knowledge) and transform it into their 
own internal representation, based on their personal knowledge-base.

Relying on common representations, eliding definitions, projecting and 
transforming knowledge are fundamental to the way humans communicate. They are 
readily observable in all forms of communications, whether written or oral, as 
we assign meaning to given sounds and symbols (words) according to the agreed 
upon grammar of a given language and use those words (knowledge projections) to 
simplify communication to a given audience. A context-specific glossary, or 
more generally a dictionary, is a prime example of a knowledge-base that we use 
for communication via knowledge projections and transformations. By maintaining 
a shared vocabulary, we can communicate using the symbolic representations 
(words) instead of requiring terms to be decomposed (defined) to their most 
basic form. However, communication of this sort is still imperfect, due to gaps 
in shared knowledge between participants or misunderstanding of overloaded 
terms. Interpersonal communications can involve nuance and context-dependent 
interpretations, yet they still boil down to knowledge projection on the part of
the communicator and knowledge transformation on the part of the 
communicatee.\footnote{All this to say, words are a shorthand for their 
definitions.} The latter can infer context, or be provided with explicit 
context, which affirms their use of the appropriate knowledge transformations.

Returning to the context of software systems, if we broaden our view from a 
single system, to a software family, we can also find patterns of commonality 
and repeated knowledge across the various \sfs{} of the family members (For 
example our SWHS and NoPCM case studies) as they have been developed to solve 
similar, or in our case nearly identical, problems. Software family members are 
good examples to help determine what types of information or knowledge provided 
in the \sfs{} belong to the system-domain, problem-domain, or are simply 
general (boilerplate).

\fig{
\centering
\includegraphics[width=\textwidth]{figures/swhs_TM1.png}}
{Theoretical Model of conservation of thermal energy found in both the SWHS and 
NoPCM SRS}
{fig:swhstm1}

Looking at SWHS and NoPCM, we can easily find identical theoretical models 
(TMs) as the underlying theory for each system is based on the problem domain 
(see example in Figure~\ref{fig:swhstm1}).
However, when we follow the derivations from the TMs to the Instance Models 
(IMs), we find the resulting equations have changed due to the context of the 
system; the lack of PCM has changed the relevant equations for calculating the 
energy balance on water in the tank as shown in Figure~\ref{fig:swhsnopcmim1}.

\fig{
\centering
%\emph{Figure showing the Ref Section of one case 
%			study, split into multiple subfigures - case study TBD}
\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=0.8\textwidth]{figures/swhs_IM1.png}}
\caption{SWHS Instance Model for Energy Balance on Water}
\label{fig:swhs_im1}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\fbox{\includegraphics[width=0.8\textwidth]{figures/nopcm_IM1.png}}
\caption{NoPCM Instance Model for Energy Balance on Water}
\label{fig:nopcm_im1}
\end{subfigure}
}
{Instance Model difference between SWHS and NoPCM }
{fig:swhsnopcmim1}

While these examples are fairly small and specific, they are indicative of 
a larger, more generalizable, set of patterns of knowledge organization and 
repetition. These patterns are at their core: the use of common knowledge that 
has been projected through some means, and patterns of organization of those 
knowledge projections within \sfs{}. Common knowledge, in this case, refers to 
one of three categories of knowledge: system-specific, domain-specific, or 
common to \sfs{} as a whole. It should also be noted that knowledge projections 
may include the identity projection (ie. the full, unabridged definition) as 
they are dependent on the relevance to the audience of the given \sf{}. 
Regardless, the captured knowledge fundamentally underlies these patterns of 
repetition, and is where we need to focus if we intend to reduce unnecessary 
redundancy and improve maintainability.

These patterns directly inform the requirements and architecture of Drasil. By 
understanding them, we reveal challenges and opportunities for automation and 
consistency in software artifact generation. Recognizing that redundancy arises 
from projecting the same core knowledge into different forms for different 
audiences, we see the need for a framework that can capture, organize, and 
project knowledge in a principled, reusable way. This insight is the conceptual 
bridge to Drasil: a framework explicitly designed to operationalize these 
patterns, reduce unnecessary duplication, and ensure consistency across 
artifacts. The following sections build on this foundation.

\section{Organizing knowledge - a fluid approach}

Recognizing patterns of redundancy and knowledge projection, we are left with a 
crucial question: how should knowledge itself be captured and organized to 
support consistent, flexible reuse across artifacts? While our analysis has 
revealed the importance of knowledge projections and the risks of redundancy, 
it remains unclear what form the underlying knowledge base should take, or how 
it should be structured to enable effective projection. In the following 
section, we turn to this open problem, exploring conceptual approaches to 
knowledge capture and organization that can lay the groundwork for Drasil's 
design.

To this point we have been referring to an intuitive, albeit nebulous concept 
of \emph{knowledge} without explicitly defining it. Before we can discuss how 
to organize knowledge, let us first clarify what it is: knowledge is 
information that has been structured and contextualized to be meaningful and 
useful. It encompasses facts, concepts, definitions, theories, and 
relationships that are relevant to a particular domain or problem. Knowledge 
can be represented in various forms, such as mathematical equations, natural 
language descriptions, code snippets, diagrams, and more. The core of what a 
piece of knowledge is remains consistent, regardless of its representation. 
This should become more clear in Chapter~\ref{c:drasil}, specifically 
Section~\ref{sec:kcChunk}.

\emph{For clarity in the following examples, we will use set notation to 
describe relationships between knowledge and projections. This is not intended 
as a formalization, but rather as a familiar mathematical juxtaposition to help 
illustrate the concepts.} Here, we use $K$ to denote a set, or in our 
terminology a \emph{chunk}, of one or more related pieces of information. Each 
$K$ may represent a single fact, a definition, or a tightly connected set of 
information relevant to a particular concept or artifact.

Given the patterns across \sfs{} we observed in the previous section, we can 
generalize knowledge projections to their projection functions. Identity 
projection functions directly repeat knowledge verbatim i.e. $p(K) \equiv K$ 
for some set of knowledge $K$. It should be noted in our case that as long as 
the projection used contains the full definition and context of a piece of 
knowledge, that projection is considered an identity projection 
\textbf{regardless of changes to notation} (ex. ``$x = y$" vs ``$y = x$") or 
language (ex. ``$x = y$" vs ``$x$ is equal to $y$" vs ``x est \'egal \`a y"). 
An identity projection is providing the knowledge chunk in full. 
Non-identity projections require using representations that elide some details 
of the knowledge at hand to make it more palatable for the audience of the 
given \sf{} i.e. $p(K) \subset K$. Similarly, we can have multivariant 
projection functions (whether identity or not) which project knowledge from 
several places into one form, i.e. $p(K,L,M) \subseteq K\cup L\cup M$.

In all cases, we have some fully defined knowledge and then we apply 
a projection function to retrieve the necessary information for our \sf{}. We 
can postulate that organizing and grouping knowledge into some type of 
structure, with an assortment of projection functions (ex. To look up the full 
definition in different representations, pull out relevant portions for a given 
audience, or provide useful abstractions) will allow us to reduce the need for 
manual duplication and remove unnecessary redundancies, as anything we need to 
include in our \sfs{} can be retrieved from a given source with a given 
projection function.

Keeping in mind that our core knowledge is used across all \sfs{} via 
projections, we may naively choose to consolidate all knowledge into a single
database. This naive approach works well enough for a limited set of examples, 
but it quickly becomes apparent (refer to the PM example from 
Section~\ref{sec:patterns}) that context is highly important and the sheer 
scope of knowledge to be organized may become unwieldy. After breaking down 
multiple case studies, we believe collecting knowledge chunks into categories 
based on their domain(s) is a more easily navigable and maintainable approach. 
This also allows us to keep some context information at a meta-level (ex. 
Physics knowledge would be categorized into a Physics knowledge-base). Then for 
any given system, we would likely only need to reference across a handful of 
contexts (knowledge-bases) relevant to the domain.

There is also knowledge fundamental to all \sfs{}\textemdash{}it is contextual 
to the domain of \sf{} writing itself. This kind of meta-knowledge would be 
useful to have readily available in its own knowledge-base. The same could be 
said for things like SI Unit definitions, while they only apply to measuring 
physical properties, we see some domains built off of physics that operate at a 
higher level of assumed understanding (ex. Chemistry abstracts some of the 
physics details, while being directly reliant on them).

Some of the knowledge used in our \sfs{} is derived from other, more 
fundamental, knowledge chunks. For example, when using SI units, we may choose 
to use a derived unit (newton, joule, radian, etc.) that is a better fit for 
the application domain of the system being documented. While we want to avoid 
unnecessary redundancy, we can argue that derived units are good candidates for 
acceptable redundancy. For example, if anywhere we use \emph{Joules} we replace 
that with the definition ($J = \frac{kg\cdot{}m^2}{s^2}$) we then run into a 
problem of context and complexity. Generally, the audience for a given \sf{} 
will have an internal representation of context-specific knowledge, so even 
something as straightforward as changing the units from $J$ to 
$\frac{kg\cdot{}m^2}{s^2}$ will put unnecessary load on said audience and force 
them to engage in more intensive knowledge transformations, while also 
potentially making the \sfs{} harder to parse for experts in the domain. In 
these cases, we want to use the derived knowledge in place of the core 
knowledge.

Being able to specify our level of abstraction through the progressive 
application of projection functions eludes to another necessary piece of 
knowledge organization: the projection functions themselves. As we project out 
core knowledge that we know of as otherwise commonly derived concepts (like the 
Joule example above), we should also like to store them in some form. For 
example, derived units may end up in the same context as the SI Units, defined 
by specific projection functions applied to SI Units. Continuing the Joule 
example, we would be applying a projection function across core knowledge 
related to energy and specific SI Units, then calling that projection 
\emph{Joule} and giving it a symbolic representation $J$ that we can refer to 
later.

We want to take a fluid and practical approach to organizing knowledge, such 
that we can keep domain-related knowledge chunks together with useful 
derivations. We want to separate knowledge in unrelated domains, such that it 
is straightforward to look up whatever we need with relative ease. The 
specific implementation for organization will be detailed later (See 
Chapter~\ref{sec:kc}).

In summary, this approach to organizing knowledge (grouping related information 
into domain-specific knowledge-bases and supporting flexible projection) lays 
the conceptual foundation for Drasil's framework. By making explicit how 
knowledge is structured and accessed, we address the root causes of redundancy 
and inconsistency identified earlier in this chapter. This organizational 
strategy not only clarifies the requirements for knowledge capture and reuse, 
but also directly informs the design principles and implementation choices 
discussed later in the thesis.

\section{Summary - The seeds of Drasil}

This final section synthesizes the chapter's findings, highlighting the key 
insights that inform the design of Drasil. By summarizing our process and 
rationale, we prepare the reader for the transition from analysis to 
implementation in the following chapter.

Through this chapter, as part of our effort to reduce unnecessary redundancy 
across the software development process, we have taken an approach to breaking 
down \sfs{} to the core knowledge they present and looked for commonalities in 
that knowledge between them. We use several case study systems that fit our 
scope (\mbox{input~$\rightarrow$~process~$\rightarrow$~output}) as examples to 
give a concrete, applied base to the work.

Generally, we see \sfs{} for a given system have a lot in common, namely they 
require the same core knowledge tailored to a specific audience for each \sf{}. 
This knowledge is organized in a meaningful way, and portions relevant to the 
context of the \sf{} are presented to the audience.

We delved into the idea of knowledge chunks and projection functions for 
producing context-relevant pieces of knowledge that are consumable by a given 
audience. We have also explored strategies for organizing that knowledge in a 
practical manner. 

We have determined the three main components necessary for any useful \sf{}: 
knowledge, context (ie. audience), and organizational structure. From here we 
can operationalize each component in a reusable and (relatively) 
redundancy-free manner. This operationalization informs the initial design for 
our framework Drasil which will be covered in depth in Chapter~\ref{c:drasil}. 

Effectively, we want to automate the generation of \sfs{} through applying 
projections to knowledge and presenting it in a given structure. The structure 
of \sfs{} is relatively straightforward to deal with, we can use templates, 
blueprints, or deterministic generation which rely on relatively common 
technologies. The knowledge-capture and projection is much more interesting as 
it relies on some yet-to-be-determined knowledge-capture mechanism that can 
provide us with chunks of knowledge that can then be fed to projection 
functions in some context-aware manner.
